{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andentze/FaceColab_Unofficial/blob/main/FaceColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7-0WoBC71P8"
      },
      "source": [
        "# Faceswap Notebook\n",
        " \n",
        "Welcome to the Faceswap *not official* notebook. Ever had a problem with your PC/laptop not being good enough for Faceswap? This notebook will help you out with **everything** regarding Faceswap.\n",
        " \n",
        " \n",
        "Feel free to set up the environment as you like. I, personally, prefer my dataset and the model being on the Google Drive directly, or something like this:\n",
        " \n",
        "> \"/content/drive/My Drive/colab/faceswap/model\n",
        " \n",
        "> \"/content/drive/My Drive/colab/faceswap/faces/A\n",
        " \n",
        "> /content/drive/My Drive/colab/faceswap/faces/B\n",
        " \n",
        "> /content/drive/My Drive/colab/faceswap/config\n",
        " \n",
        "and so on.\n",
        " \n",
        "You can choose to either use the GUI version of Faceswap(*which can take some time to set up*) or the CLI version of Faceswap that is present here.\n",
        " \n",
        "You will also receive 4 kinds of GPUs: P4, T4, K80 or P100(*different kinds of GPU's in Colab Pro which are proven to be faster*). Also, you get 12 hours of notebook usage until you get disconnected(24 hours in Colab Pro). You *might* get frequent disconnects.\n",
        " \n",
        "And before you start using this notebook, clone it to your Google Drive. (File -> Save a copy in Drive)\n",
        "\n",
        "**I TAKE NO CREDIT FOR ANY REPOSITORIES, NOR SOFTWARE USED IN THIS NOTEBOOK. ANYTHING USED HERE IS NOT MINE.**\n",
        "\n",
        "*If you wish to support me by buying me a donut, feel free to [donate](https://www.donationalerts.com/r/andentze).* You *may* get early access to the notebook updates, which I'm not really sure is needed. But at the end of the day, it's one way to support me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7TctjDRl8d3N"
      },
      "outputs": [],
      "source": [
        "#@title Check the current GPU\n",
        "#@markdown Right here you can check what kind of GPU you have available right now. Run this code to output the GPU used.\n",
        "#@markdown If it errors out, make sure your runtime type is set to \"GPU\".\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "btFh5tBGExWQ"
      },
      "outputs": [],
      "source": [
        "#@title Keep-Alive Script\n",
        "#@markdown You have to activate this to automatically reconnect if Colab disconnects you. Run this code, to activate the \"Keep-Alive\" script.\n",
        "#@markdown And just to make sure, open the console using Ctrl+Shift+I, and paste the code below. This will work efficently for both CLI and DE.\n",
        " \n",
        "import IPython\n",
        "from google.colab import output\n",
        " \n",
        "display(IPython.display.Javascript('''\n",
        "function ClickConnect() {\n",
        "  console.log('Working')\n",
        "  document\n",
        "    .querySelector('#top-toolbar > colab-connect-button')\n",
        "    .shadowRoot.querySelector('#connect')\n",
        "    .click()\n",
        "}\n",
        " \n",
        "setInterval(ClickConnect, 60000)\n",
        "'''))\n",
        " \n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnAN0NQAGe49"
      },
      "source": [
        "> function ClickConnect() {\n",
        "\n",
        ">  console.log('Working')\n",
        "\n",
        ">  document\n",
        "\n",
        ">    .querySelector('#top-toolbar > colab-connect-button')\n",
        "\n",
        ">    .shadowRoot.querySelector('#connect')\n",
        "\n",
        ">    .click()\n",
        "}\n",
        "\n",
        ">    setInterval(ClickConnect, 60000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcGpeieHXHb"
      },
      "source": [
        "# Setting up the CLI\n",
        "\n",
        "Not a fan of the GUI or it simply doesn't work? Look no furher. From this point you will install Faceswap directly to Colab without the need of the DE.\n",
        "\n",
        "It was set up so even a toddler could figure out how it works.\n",
        "\n",
        "(*only pick one of the variants of installation*)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variant 1:"
      ],
      "metadata": {
        "id": "YiY5CqcT9W01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Faceswap and base dependencies\n",
        "\n",
        "#@markdown Since Faceswap got official support of Tensorflow 2.8, running this will be enough for main functionality of Faceswap.\n",
        "\n",
        "#@markdown This will take around a minute.\n",
        "\n",
        "#@markdown If you wish to copy your config files from your Drive, feel free to fill these in.\n",
        "from IPython.display import clear_output\n",
        "\n",
        "train = \"train.ini dir(/content/dir/train.ini)\" #@param {type:\"string\"}\n",
        "extract = \"extract.ini dir(/content/dir/extract.ini)\" #@param {type:\"string\"}\n",
        "convert = \"convert.ini dir(/content/dir/convert.ini)\" #@param {type:\"string\"}\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/deepfakes/faceswap\n",
        "clear_output()\n",
        "\n",
        "!cp \"{train}\" faceswap/config/\n",
        "!cp \"{extract}\" faceswap/config/\n",
        "!cp \"{convert}\" faceswap/config/\n",
        "clear_output()\n",
        "\n",
        "%env FACESWAP_BACKEND = \"nvidia\"\n",
        "!pip install -r \"/content/faceswap/requirements/_requirements_base.txt\"\n",
        "clear_output()\n",
        "\n",
        "print(\"You are good to go.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GRaa4GUuFIsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variant 2:"
      ],
      "metadata": {
        "id": "ixhbeUeI9apF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Faceswap and Conda\n",
        "#@markdown This will install Conda and restart the runtime.\n",
        "#@markdown After the runtime restart, install dependencies below.\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Installing Faceswap.\")\n",
        "!git clone https://github.com/deepfakes/faceswap\n",
        "clear_output()\n",
        "\n",
        "print(\"Installing Conda.\")\n",
        "!pip install condacolab\n",
        "import condacolab as cc\n",
        "cc.install()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ecn4CicX5lkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "#@markdown As said before, install dependencies after the runtime restart.\n",
        "\n",
        "#@markdown You can also fill these in to copy your config files.\n",
        "\n",
        "train = \"train.ini dir(/content/dir/train.ini)\" #@param {type:\"string\"}\n",
        "extract = \"extract.ini dir(/content/dir/extract.ini)\" #@param {type:\"string\"}\n",
        "convert = \"convert.ini dir(/content/dir/convert.ini)\" #@param {type:\"string\"}\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!cp \"{train}\" faceswap/config/\n",
        "!cp \"{extract}\" faceswap/config/\n",
        "!cp \"{convert}\" faceswap/config/\n",
        "clear_output()\n",
        "print(\"Config files copied.\")\n",
        "\n",
        "print(\"Installing Conda packages.\")\n",
        "!conda install -c conda-forge tqdm==4.64.0 \\\n",
        "numpy==1.21.5 \\\n",
        "Pillow==9.0.1 \\\n",
        "scikit-learn==1.0.2 \\\n",
        "fastcluster==1.2.6 \\\n",
        "matplotlib==3.2.2 \\\n",
        "imageio==2.9.0 \\\n",
        "imageio-ffmpeg==0.4.7 \\\n",
        "cudatoolkit=11.2 cudnn=8.1\n",
        "clear_output()\n",
        "\n",
        "print(\"Installing Pip packages.\")\n",
        "!pip install pip install \"nvidia-ml-py<11.515\" \n",
        "!pip install pip install \"opencv-python>=4.5.5.0\" \n",
        "!pip install pip install \"ffmpy==0.2.3\" \n",
        "!pip install pip install \"psutil>=5.8.0\" \n",
        "!pip install pip install \"tensorflow-gpu>=2.2.0,<2.9.0\"\n",
        "clear_output()\n",
        "\n",
        "print(\"You are good to go.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4mRiDrl96NGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvi8sfssLKR8"
      },
      "source": [
        "And with that, you're good to go! You can go ahead, and use Faceswap at your own will!(*that is, until you hit a 12 hour mark.*)\n",
        "\n",
        "**It appears Colab doesn't like long-terms calculations. So it will disconnect you at the most random times. BE AWARE OF THAT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgIDaoDPemKr"
      },
      "source": [
        "# Workflow\n",
        " \n",
        "Welcome to the Faceswap environment. Your first step is to extract the faces from your footage. After which you're going to have to fix your dataset. While PC/laptop users can fix that locally, mobile users will have to use the DE to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zta3jdNf7Wlf"
      },
      "outputs": [],
      "source": [
        "#@title Extraction\n",
        "#@markdown Extraction is the first step for dataset creation. You **must** have good data in order for training to work better. Use this code snippet to extract your dataset.\n",
        " \n",
        "#set variables start\n",
        " \n",
        "input_dir = \"/content/dir/to/video.mp4\" #@param {type:\"string\"}\n",
        "output_dir = \"/content/dir/to/faces/folder\" #@param {type:\"string\"}\n",
        "detector = \"s3fd\" #@param [\"cv2-dnn\", \"mtcnn\", \"s3fd\"]\n",
        "alignment = \"fan\" #@param [\"cv2dnn\", \"fan\"]\n",
        "masker = \"bisenet-fp\" #@param [\"bisenet-fp\", \"unet-dfl\", \"vgg-clear\", \"vgg-obstructed\"]\n",
        "al_normalization = \"hist\" #@param [\"clahe\", \"hist\", \"mean\"]\n",
        "refeed = 4 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "size = 512 #@param {type:\"slider\", min:256, max:2048, step:64}\n",
        "extract_every_n = 1 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "#set variables end\n",
        "!python3 '{path}'faceswap.py extract \\\n",
        "-i '{input_dir}' \\\n",
        "-o '{output_dir}' \\\n",
        "-D '{detector}' \\\n",
        "-A '{alignment}' \\\n",
        "-nm '{al_normalization}' \\\n",
        "-M '{masker}' \\\n",
        "-rf '{refeed}' \\\n",
        "-min 20 -l 0.4 \\\n",
        "-sz '{size}' \\\n",
        "-een '{extract_every_n}' \\\n",
        "-si 0 -ssf -L INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "twHA2JGjzPXB"
      },
      "outputs": [],
      "source": [
        "#@title Extraction with saving faces\n",
        "#@markdown Extraction is the first step for dataset creation. You **must** have good data in order for training to work better. Use this code snippet to extract your dataset.\n",
        "\n",
        "#@markdown The script above extracts each face and does not save faces to the directory. Use this code to actually save faces and tinker around with them.\n",
        "\n",
        "#set variables start\n",
        "\n",
        "!python3 '{path}'faceswap.py extract \\\n",
        "-i '{input_dir}' \\\n",
        "-o '{output_dir}' \\\n",
        "-D '{detector}' \\\n",
        "-A '{alignment}' \\\n",
        "-M '{masker}' \\\n",
        "-nm '{al_normalization}' \\\n",
        "-rf '{refeed}' \\\n",
        "-min 20 -l 0.4 \\\n",
        "-sz '{size}' \\\n",
        "-een '{extract_every_n}' \\\n",
        "-si 0 -L INFO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip the dataset\n",
        "#@markdown If you have the dataset ready to use on your PC and it's quite massive, it's *much* better to pack it to the .zip file and upload it to the Drive.\n",
        "\n",
        "zip_directory = \"/content/dir/to/file.zip\" #@param {type:\"string\"}\n",
        "output_directory = \"/content/dir/to/faces/A or B\" #@param {type:\"string\"}\n",
        "#@markdown Output directory depends on which side of the face are you unpacking. Do note that it may take a while to unpack and may heavily fill your Drive storage.\n",
        "\n",
        "#@markdown The original .zip file will be deleted right after the unpacking is finished.\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!unzip '{zip_directory}' -d '{output_directory}'\n",
        "!rm '{zip_directory}'\n",
        "clear_output()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P7kIctLDr40I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6D42AWnZUc6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training\n",
        "#@markdown Now, you have your dataset at tip-top shape! All you have to do is start training now! But wait. Before you actually start, make sure your configuration was set right.\n",
        " \n",
        "#@markdown Made sure? Then get to training by setting up the variables first.\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#set variables start\n",
        "input_a = \"/content/dir/to/faces/A\" #@param {type:\"string\"}\n",
        "input_b = \"/content/dir/to/faces/B\" #@param {type:\"string\"}\n",
        "num_iterations = 100000 #@param {type:\"slider\", min:100000, max:3000000, step:25000}\n",
        "save_every = 360 #@param {type:\"slider\", min:360, max:1800, step:360}\n",
        "save_model_every = 25000 #@param {type:\"slider\", min:25000, max:100000, step:25000}\n",
        "batch_num = 16 #@param {type:\"slider\", min:4, max:72, step:4}\n",
        "trainer_type = \"dfaker\" #@param [\"lightweight\", \"original\", \"iae\", \"dfaker\", \"dlight\", \"unbalanced\", \"dfl-h128\", \"villain\", \"phaze-a\"] \n",
        "model_dir = \"/content/dir/to/model\" #@param {type:\"string\"}\n",
        "timelapse_dir = \"/content/dir/to/timelapse\" #@param {type:\"string\"}\n",
        "\n",
        "#set variables end\n",
        "\n",
        "print(\"Please wait, while it pre-loads the A dataset.\")\n",
        "print(\"This may take a while depending on how many faces you have.\")\n",
        "print(\"Pre-loading is needed to load EVERY image in the directory for Faceswap.\")\n",
        "print(\"If the dataset is not pre-loaded, some images may not load into the model.\")\n",
        "!dir '{input_a}'\n",
        "clear_output()\n",
        "print(\"Please wait, while it pre-loads the B dataset.\")\n",
        "print(\"This may take a while depending on how many faces you have.\")\n",
        "print(\"Pre-loading is needed to load EVERY image in the directory for Faceswap.\")\n",
        "print(\"If the dataset is not pre-loaded, some images may not load into the model.\")\n",
        "!dir '{input_b}'\n",
        "clear_output()\n",
        "\n",
        "   #fit training args: -nw -nf\n",
        " \n",
        "!python3 '{path}'/faceswap.py train \\\n",
        "  -A '{input_a}' \\\n",
        "  -B '{input_b}' \\\n",
        "  -m '{model_dir}' \\\n",
        "  -t '{trainer_type}' \\\n",
        "  -bs '{batch_num}' \\\n",
        "  -it '{num_iterations}' \\\n",
        "  -s '{save_every}' \\\n",
        "  -ss '{save_model_every}' \\\n",
        "  -tia '{input_a}' \\\n",
        "  -tib '{input_b}' \\\n",
        "  -to '{timelapse_dir}' \\\n",
        "  -w "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YnHtwcncTuOd"
      },
      "outputs": [],
      "source": [
        "#@title Training with No Warp\n",
        "#@markdown Disabling the warp can let you achieve extra sharpness. Start this training sequence if you think your model has trained enough(*judge by the previews*).\n",
        "#@markdown Your variables are saved from the training session above. If not, start your normal training sequence first, then terminate it.\n",
        "\n",
        "\n",
        "!python3 '{path}'/faceswap.py train \\\n",
        "  -A '{input_a}' \\\n",
        "  -B '{input_b}' \\\n",
        "  -m '{model_dir}' \\\n",
        "  -t '{trainer_type}' \\\n",
        "  -bs '{batch_num}' \\\n",
        "  -it '{num_iterations}' \\\n",
        "  -s '{save_every}' \\\n",
        "  -ss '{save_model_every}' \\\n",
        "  -tia '{input_a}' \\\n",
        "  -tib '{input_b}' \\\n",
        "  -to '{timelapse_dir}' \\\n",
        "  -w \\\n",
        "  -nw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ORdq3Tx1lgAu"
      },
      "outputs": [],
      "source": [
        "#@title Convert\n",
        "#@markdown After your model is trained enough, you can convert your final result. Fill the variables and run the code yet another time.\n",
        "\n",
        "#variables set\n",
        "input_video = \"/content/dir/videoA.mp4\" #@param {type:\"string\"}\n",
        "output_dir = \"/content/dir/output\" #@param {type:\"string\"}\n",
        "alignments = \"/content/dir/alignments.fsa\" #@param {type:\"string\"}\n",
        "model_dir = \"/content/dir/your_model\" #@param {type:\"string\"}\n",
        "color_adj = \"\" #@param [\"\", \"avg-color\", \"color-transfer\", \"manual-balance\", \"match-hist\", \"seamless-clone\"]\n",
        "mask = \"\" #@param [\"\", \"components\", \"extended\", \"unet-dfl\", \"vgg-clear\", \"vgg-obstructed\", \"predicted\", \"bisenet-fp_face\", \"bisenet-fp_head\"]\n",
        "writer = \"ffmpeg\" #@param [\"ffmpeg\", \"gif\", \"opencv\", \"pillow\"]\n",
        "output_scale = 100 #@param {type:\"slider\", min:25, max:400, step:5}\n",
        "#@markdown \"Predicted\" mask type is used only if you trained your own mask using \"Learn Mask\". If you're not sure, use \"Components\" or \"Extended\" as they are used by default.\n",
        "#variables end\n",
        "\n",
        "!python3 faceswap/faceswap.py convert \\\n",
        "-i '{input_video}' \\\n",
        "-o '{output_dir}' \\\n",
        "-al '{alignments}' \\\n",
        "-m '{model_dir}' \\\n",
        "-c '{color_adj}' \\\n",
        "-M '{mask}' \\\n",
        "-w '{writer}' \\\n",
        "-osc '{output_scale}' -l 0.4 -j 0 -L INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PYMlgKBSw8iZ"
      },
      "outputs": [],
      "source": [
        "#@title Convert with Swap Model\n",
        "#@markdown Noticed that you trained the model the other way around? Do not worry. Run this to convert it the other way around.\n",
        "\n",
        "#@markdown The variables should stay the same as from the code above.\n",
        "\n",
        "!python3 faceswap/faceswap.py convert \\\n",
        "-i '{input_video}' \\\n",
        "-o '{output_dir}' \\\n",
        "-al '{alignments}' \\\n",
        "-m '{model_dir}' \\\n",
        "-c '{color_adj}' \\\n",
        "-M '{mask}aa' \\\n",
        "-w '{writer}' \\\n",
        "-osc '{output_scale}' -s -l 0.4 -j 0 -L INFO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqe2bTXRKjGC"
      },
      "source": [
        "# Various Tools\n",
        "\n",
        "*Use if necessary.*\n",
        "\n",
        "Faceswap provides a couple of handy tools for you to make your dataset managment easier. Here you can do whatever you want(*limited to the \"Manual\" tool as it requires a desktop environment*)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faceswap Tools\n",
        "\n",
        "These are the most handy tools to use for managing the dataset using Faceswap."
      ],
      "metadata": {
        "id": "JkG_dSBnvcau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "83ydXHCnqjDR"
      },
      "outputs": [],
      "source": [
        "#@title Alignments\n",
        "#@markdown This tool allows you to perform various actions to the alignments file. It is usually stored next to the video source file.\n",
        "#@markdown You would only need \"Remove Faces\" job for most of the time.\n",
        "#@markdown If you wish to use any other tool, proceed to the guides to find out how to use some of them.\n",
        "#set variables\n",
        "job = \"remove-faces\" #@param [\"draw\", \"extract\", \"missing-alignments\", \"missing-frames\", \"multi-faces\", \"no-faces\", \"remove-faces\", \"rename\", \"sort\"]\n",
        "alignments = \"/content/dir/alignments.fsa\" #@param {type:\"string\"}\n",
        "faces = \"/content/dir/A or B\" #@param {type:\"string\"}\n",
        "drive_install = False #@param {type:\"boolean\"}\n",
        "if drive_install:\n",
        "  path = \"/content/drive/My Drive/faceswap/\"\n",
        "  print(\"The installation is on Google Drive. Using Drive directory.\")\n",
        "if not drive_install:\n",
        "  path = \"/content/faceswap/\"\n",
        "  print(\"The installation is local. Using local directory.\")\n",
        "#end variables\n",
        "!python3 faceswap\\tools.py alignments \\\n",
        "-j '{job}' \\\n",
        "-o console \\\n",
        "-a '{alignments}' \\\n",
        "-fc '{faces}' \\\n",
        "-een 1 -sz 512 -L INFO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td8VqXsuq8S4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title FFmpeg\n",
        "#@markdown Generate videos for timelapse(*usually this tool is required for that*). **Strongly** recommended to use BRU first to rename each timestamp to start from 000000.jpeg.\n",
        "\n",
        "#set variables\n",
        "input = \"your/timelapse/dir/here\" #@param {type:\"string\"}\n",
        "output = \"your/video/output/here/example.mp4\" #@param {type:\"string\"}\n",
        "fps = 5 #@param {type:\"slider\", min:1, max:60}\n",
        "\n",
        "#end variables\n",
        "!python3 faceswap\\tools.py effmpeg \\\n",
        "-a gen-vid \\\n",
        "-i input \\\n",
        "-o output \\\n",
        "-fps fps \\\n",
        "-ef .png \\\n",
        "-s 00:00:00 \\\n",
        "-e 00:00:00 \\\n",
        "-d 00:00:00 -sc 1920x1080 -L INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YjpMdDFktO-m"
      },
      "outputs": [],
      "source": [
        "#@title Sort\n",
        "\n",
        "#@markdown This is the Sort tool. It helps you with cleaning your dataset more efficiently.\n",
        "\n",
        "#@markdown The sorting may require **a lot** of RAM depending on your dataset quantity. The bigger your dataset is, the more RAM you may need.\n",
        "\n",
        "#@markdown The formula for calculating the ammount of RAM is: *(nÂ² * 24) / 1.8* where *n* is the number of images.\n",
        "#@markdown The number you'll get is in bytes, so divide it by 1024 for megabytes, and by 1024 again to get gigabytes.\n",
        "\n",
        "#@markdown If the sorting fails(*i.e. the OOM issue*), divide the dataset in some small parts and sort each one of them.\n",
        "\n",
        "#@markdown Please note, that there are only those jobs, that **I** think are useful.\n",
        "input = \"/content/drive/MyDrive/colab_files/faceswap/faces/B\" #@param{type:\"string\"}\n",
        "job = \"face-yaw\" #@param [\"face\", \"blur\", \"hist\", \"face-yaw\"]\n",
        "\n",
        "!python3 faceswap/tools.py sort \\\n",
        "-i '{input}' \\\n",
        "-s '{job}' \\\n",
        "-t -1.0 -fp rename -g hist -b 5 -lf sort_log.json -L INFO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config\n",
        "\n",
        "Here you can change the config files. Only change the orange text.\n",
        "\n",
        "**WARNING: HUGE WALL OF TEXT AHEAD**"
      ],
      "metadata": {
        "id": "OL9welPIMdKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extract.ini"
      ],
      "metadata": {
        "id": "sWnIRM0tNd_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = \"\"\"\n",
        "[global]\n",
        "# OPTIONS THAT APPLY TO ALL EXTRACTION PLUGINS\n",
        "\n",
        "# [Nvidia Only]. Enable the Tensorflow GPU `allow_growth` configuration option. This option prevents\n",
        "# Tensorflow from allocating all of the GPU VRAM at launch but can lead to higher VRAM fragmentation\n",
        "# and slower performance. Should only be enabled if you are having problems running extraction.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "allow_growth = False\n",
        "\n",
        "[align.fan]\n",
        "# FAN ALIGNER OPTIONS.\n",
        "# FAST ON GPU, SLOW ON CPU. BEST ALIGNER.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "#     - AMD users: A batchsize of 8 requires about 4 GB vram.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 12]\n",
        "batch-size = 12\n",
        "\n",
        "[detect.cv2_dnn]\n",
        "# CV2 DNN DETECTOR OPTIONS.\n",
        "# A CPU ONLY EXTRACTOR, IS THE LEAST RELIABLE, BUT USES LEAST RESOURCES AND RUNS FAST ON CPU. USE THIS\n",
        "# IF NOT USING A GPU AND TIME IS IMPORTANT\n",
        "\n",
        "# The confidence level at which the detector has succesfully found a face.\n",
        "# Higher levels will be more discriminating, lower levels will have more false positives.\n",
        "# \n",
        "# Select an integer between 25 and 100\n",
        "# [Default: 50]\n",
        "confidence = 50\n",
        "\n",
        "[detect.mtcnn]\n",
        "# MTCNN DETECTOR OPTIONS.\n",
        "# FAST ON GPU, SLOW ON CPU. USES FEWER RESOURCES THAN OTHER GPU DETECTORS BUT CAN OFTEN RETURN MORE\n",
        "# FALSE POSITIVES.\n",
        "\n",
        "# The minimum size of a face (in pixels) to be accepted as a positive match.\n",
        "# Lower values use significantly more VRAM and will detect more false positives.\n",
        "# \n",
        "# Select an integer between 20 and 1000\n",
        "# [Default: 20]\n",
        "minsize = 20\n",
        "\n",
        "# The scale factor for the image pyramid.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 0.9\n",
        "# [Default: 0.709]\n",
        "scalefactor = 0.709\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 8]\n",
        "batch-size = 8\n",
        "\n",
        "# First stage threshold for face detection. This stage obtains face candidates.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 0.9\n",
        "# [Default: 0.6]\n",
        "threshold_1 = 0.6\n",
        "\n",
        "# Second stage threshold for face detection. This stage refines face candidates.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 0.9\n",
        "# [Default: 0.7]\n",
        "threshold_2 = 0.7\n",
        "\n",
        "# Third stage threshold for face detection. This stage further refines face candidates.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 0.9\n",
        "# [Default: 0.7]\n",
        "threshold_3 = 0.7\n",
        "\n",
        "[detect.s3fd]\n",
        "# S3FD DETECTOR OPTIONS.\n",
        "# FAST ON GPU, SLOW ON CPU. CAN DETECT MORE FACES AND FEWER FALSE POSITIVES THAN OTHER GPU DETECTORS,\n",
        "# BUT IS A LOT MORE RESOURCE INTENSIVE.\n",
        "\n",
        "# The confidence level at which the detector has succesfully found a face.\n",
        "# Higher levels will be more discriminating, lower levels will have more false positives.\n",
        "# \n",
        "# Select an integer between 25 and 100\n",
        "# [Default: 70]\n",
        "confidence = 70\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "#     - AMD users: A batchsize of 8 requires about 2 GB vram.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 4]\n",
        "batch-size = 4\n",
        "\n",
        "[mask.bisenet_fp]\n",
        "# BISENET FACE PARSING OPTIONS.\n",
        "# MASK PORTED FROM HTTPS://GITHUB.COM/ZLLRUNNING/FACE-PARSING.PYTORCH.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 8]\n",
        "batch-size = 8\n",
        "\n",
        "# The trained weights to use.\n",
        "# \n",
        "#     - faceswap - Weights trained on wildly varied Faceswap extracted data to better handle varying\n",
        "# \t\tconditions, obstructions, glasses and multiple targets within a single extracted image.\n",
        "#     - original - The original weights trained on the CelebAMask-HQ dataset.\n",
        "# \n",
        "# Choose from: ['faceswap', 'original']\n",
        "# [Default: faceswap]\n",
        "weights = faceswap\n",
        "\n",
        "# Whether to include ears within the face mask.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "include_ears = False\n",
        "\n",
        "# Whether to include hair within the face mask.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "include_hair = False\n",
        "\n",
        "# Whether to include glasses within the face mask.\n",
        "#     - For 'original' weights excluding glasses will mask out the lenses as well as the frames.\n",
        "#     - For 'faceswap' weights, the model has been trained to mask out lenses if eyes cannot be seen\n",
        "# \t\t(i.e. dark sunglasses) or just the frames if the eyes can be seen.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "include_glasses = True\n",
        "\n",
        "[mask.unet_dfl]\n",
        "# UNET_DFL OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES.\n",
        "# THE MASK MODEL HAS BEEN TRAINED BY COMMUNITY MEMBERS. INSERT MORE COMMENTARY ON TESTING HERE.\n",
        "# PROFILE FACES MAY RESULT IN SUB-PAR PERFORMANCE.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 8]\n",
        "batch-size = 8\n",
        "\n",
        "[mask.vgg_clear]\n",
        "# VGG_CLEAR OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES CLEAR OF\n",
        "# OBSTRUCTIONS.\n",
        "# PROFILE FACES AND OBSTRUCTIONS MAY RESULT IN SUB-PAR PERFORMANCE.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 6]\n",
        "batch-size = 6\n",
        "\n",
        "[mask.vgg_obstructed]\n",
        "# VGG_OBSTRUCTED OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES.\n",
        "# THE MASK MODEL HAS BEEN SPECIFICALLY TRAINED TO RECOGNIZE SOME FACIAL OBSTRUCTIONS (HANDS AND\n",
        "# EYEGLASSES). PROFILE FACES MAY RESULT IN SUB-PAR PERFORMANCE.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 2]\n",
        "batch-size = 2\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"faceswap/config/extract.ini\", \"w\") as text_file:\n",
        "    text_file.write(config)"
      ],
      "metadata": {
        "id": "uXQnkefnNhFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train.ini"
      ],
      "metadata": {
        "id": "A0jMFCeJMr1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = \"\"\"\n",
        "[global]\n",
        "# OPTIONS THAT APPLY TO ALL MODELS\n",
        "# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.\n",
        "\n",
        "# How to center the training image. The extracted images are centered on the middle of the skull based\n",
        "# on the face's estimated pose. A subsection of these images are used for training. The centering used\n",
        "# dictates how this subsection will be cropped from the aligned images.\n",
        "#     - face: Centers the training image on the center of the face, adjusting for pitch and yaw.\n",
        "#     - head: Centers the training image on the center of the head, adjusting for pitch and yaw. NB:\n",
        "# \t\tYou should only select head centering if you intend to include the full head (including hair) in\n",
        "# \t\tthe final swap. This may give mixed results. Additionally, it is only worth choosing head\n",
        "# \t\tcentering if you are training with a mask that includes the hair (e.g. BiSeNet-FP-Head).\n",
        "#     - legacy: The 'original' extraction technique. Centers the training image near the tip of the\n",
        "# \t\tnose with no adjustment. Can result in the edges of the face appearing outside of the training\n",
        "# \t\tarea.\n",
        "# \n",
        "# Choose from: ['face', 'head', 'legacy']\n",
        "# [Default: face]\n",
        "centering = face\n",
        "\n",
        "# How much of the extracted image to train on. A lower coverage will limit the model's scope to a\n",
        "# zoomed-in central area while higher amounts can include the entire face. A trade-off exists between\n",
        "# lower amounts given more detail versus higher amounts avoiding noticeable swap transitions. For\n",
        "# 'Face' centering you will want to leave this above 75%. For Head centering you will most likely want\n",
        "# to set this to 100%. Sensible values for 'Legacy' centering are:\n",
        "#     - 62.5% spans from eyebrow to eyebrow.\n",
        "#     - 75.0% spans from temple to temple.\n",
        "#     - 87.5% spans from ear to ear.\n",
        "#     - 100.0% is a mugshot.\n",
        "# \n",
        "# Select a decimal number between 62.5 and 100.0\n",
        "# [Default: 87.5]\n",
        "coverage = 87.5\n",
        "\n",
        "# Use ICNR to tile the default initializer in a repeating pattern. This strategy is designed for\n",
        "# pairing with sub-pixel / pixel shuffler to reduce the 'checkerboard effect' in image reconstruction.\n",
        "#     - https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "icnr_init = False\n",
        "\n",
        "# Use Convolution Aware Initialization for convolutional layers. This can help eradicate the vanishing\n",
        "# and exploding gradient problem as well as lead to higher accuracy, lower loss and faster\n",
        "# convergence.\n",
        "# NB:\n",
        "#     - This can use more VRAM when creating a new model so you may want to lower the batch size for\n",
        "# \t\tthe first run. The batch size can be raised again when reloading the model.\n",
        "#     - Multi-GPU is not supported for this option, so you should start the model on a single GPU.\n",
        "# \t\tOnce training has started, you can stop training, enable multi-GPU and resume.\n",
        "#     - Building the model will likely take several minutes as the calculations for this\n",
        "# \t\tinitialization technique are expensive. This will only impact starting a new model.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "conv_aware_init = False\n",
        "\n",
        "# The optimizer to use.\n",
        "#     - adabelief - Adapting Stepsizes by the Belief in Observed Gradients. An optimizer with the aim\n",
        "# \t\tto converge faster, generalize better and remain more stable. (https://arxiv.org/abs/2010.07468).\n",
        "# \t\tNB: Epsilon for AdaBelief needs to be set to a smaller value than other Optimizers. Generally\n",
        "# \t\tsetting the 'Epsilon Exponent' to around '-16' should work.\n",
        "#     - adam - Adaptive Moment Optimization. A stochastic gradient descent method that is based on\n",
        "# \t\tadaptive estimation of first-order and second-order moments.\n",
        "#     - nadam - Adaptive Moment Optimization with Nesterov Momentum. Much like Adam but uses a\n",
        "# \t\tdifferent formula for calculating momentum.\n",
        "#     - rms-prop - Root Mean Square Propagation. Maintains a moving (discounted) average of the square\n",
        "# \t\tof the gradients. Divides the gradient by the root of this average.\n",
        "# \n",
        "# Choose from: ['adabelief', 'adam', 'nadam', 'rms-prop']\n",
        "# [Default: adam]\n",
        "optimizer = adam\n",
        "\n",
        "# Learning rate - how fast your network will learn (how large are the modifications to the model\n",
        "# weights after one batch of training). Values that are too large might result in model crashes and\n",
        "# the inability of the model to find the best solution. Values that are too small might be unable to\n",
        "# escape from dead-ends and find the best global minimum.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Select a decimal number between 1e-06 and 0.0001\n",
        "# [Default: 5e-05]\n",
        "learning_rate = 5e-05\n",
        "\n",
        "# The epsilon adds a small constant to weight updates to attempt to avoid 'divide by zero' errors.\n",
        "# Unless you are using the AdaBelief Optimizer, then Generally this option should be left at default\n",
        "# value, For AdaBelief, setting this to around '-16' should work.\n",
        "# In all instances if you are getting 'NaN' loss values, and have been unable to resolve the issue any\n",
        "# other way (for example, increasing batch size, or lowering learning rate), then raising the epsilon\n",
        "# can lead to a more stable model. It may, however, come at the cost of slower training and a less\n",
        "# accurate final result.\n",
        "# NB: The value given here is the 'exponent' to the epsilon. For example, choosing '-7' will set the\n",
        "# epsilon to 1e-7. Choosing '-3' will set the epsilon to 0.001 (1e-3).\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Select an integer between -20 and 0\n",
        "# [Default: -7]\n",
        "epsilon_exponent = -7\n",
        "\n",
        "# Use reflection padding rather than zero padding with convolutions. Each convolution must pad the\n",
        "# image boundaries to maintain the proper sizing. More complex padding schemes can reduce artifacts at\n",
        "# the border of the image.\n",
        "#     - http://www-cs.engr.ccny.cuny.edu/~wolberg/cs470/hw/hw2_pad.txt\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "reflect_padding = False\n",
        "\n",
        "# [Nvidia Only]. Enable the Tensorflow GPU 'allow_growth' configuration option. This option prevents\n",
        "# Tensorflow from allocating all of the GPU VRAM at launch but can lead to higher VRAM fragmentation\n",
        "# and slower performance. Should only be enabled if you are receiving errors regarding 'cuDNN fails to\n",
        "# initialize' when commencing training.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "allow_growth = False\n",
        "\n",
        "# [Nvidia Only], NVIDIA GPUs can run operations in float16 faster than in float32. Mixed precision\n",
        "# allows you to use a mix of float16 with float32, to get the performance benefits from float16 and\n",
        "# the numeric stability benefits from float32.\n",
        "# \n",
        "# While mixed precision will run on most Nvidia models, it will only speed up training on more recent\n",
        "# GPUs. Those with compute capability 7.0 or higher will see the greatest performance benefit from\n",
        "# mixed precision because they have Tensor Cores. Older GPUs offer no math performance benefit for\n",
        "# using mixed precision, however memory and bandwidth savings can enable some speedups. Generally RTX\n",
        "# GPUs and later will offer the most benefit.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "mixed_precision = False\n",
        "\n",
        "# If a 'NaN' is generated in the model, this means that the model has corrupted and the model is\n",
        "# likely to start deteriorating from this point on. Enabling NaN protection will stop training\n",
        "# immediately in the event of a NaN. The last save will not contain the NaN, so you may still be able\n",
        "# to rescue your model.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "nan_protection = True\n",
        "\n",
        "# [GPU Only]. The number of faces to feed through the model at once when running the Convert process.\n",
        "# \n",
        "# NB: Increasing this figure is unlikely to improve convert speed, however, if you are getting Out of\n",
        "# Memory errors, then you may want to reduce the batch size.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Select an integer between 1 and 32\n",
        "# [Default: 16]\n",
        "convert_batchsize = 16\n",
        "\n",
        "[global.loss]\n",
        "# LOSS CONFIGURATION OPTIONS\n",
        "# LOSS IS THE MECHANISM BY WHICH A NEURAL NETWORK JUDGES HOW WELL IT THINKS THAT IT IS RECREATING A\n",
        "# FACE.\n",
        "# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.\n",
        "\n",
        "# The loss function to use.\n",
        "#     - MAE - Mean absolute error will guide reconstructions of each pixel towards its median value in\n",
        "# \t\tthe training dataset. Robust to outliers but as a median, it can potentially ignore some\n",
        "# \t\tinfrequent image types in the dataset.\n",
        "#     - MSE - Mean squared error will guide reconstructions of each pixel towards its average value in\n",
        "# \t\tthe training dataset. As an avg, it will be susceptible to outliers and typically produces\n",
        "# \t\tslightly blurrier results.\n",
        "#     - LogCosh - log(cosh(x)) acts similar to MSE for small errors and to MAE for large errors. Like\n",
        "# \t\tMSE, it is very stable and prevents overshoots when errors are near zero. Like MAE, it is robust\n",
        "# \t\tto outliers. NB: Due to a bug in PlaidML, this loss does not work on AMD cards.\n",
        "#     - Smooth_L1 --- Modification of the MAE loss to correct two of its disadvantages. This loss has\n",
        "# \t\timproved stability and guidance for small errors.\n",
        "#     - L_inf_norm --- The L_inf norm will reduce the largest individual pixel error in an image. As\n",
        "# \t\teach largest error is minimized sequentially, the overall error is improved. This loss will be\n",
        "# \t\textremely focused on outliers.\n",
        "#     - SSIM - Structural Similarity Index Metric is a perception-based loss that considers changes in\n",
        "# \t\ttexture, luminance, contrast, and local spatial statistics of an image. Potentially delivers more\n",
        "# \t\trealistic looking images.\n",
        "#     - MS_SSIM - Multiscale Structural Similarity Index Metric is similar to SSIM except that it\n",
        "# \t\tperforms the calculations along multiple scales of the input image. NB: This loss currently does\n",
        "# \t\tnot work on AMD Cards.\n",
        "#     - GMSD - Gradient Magnitude Similarity Deviation seeks to match the global standard deviation of\n",
        "# \t\tthe pixel to pixel differences between two images. Similar in approach to SSIM. NB: This loss does\n",
        "# \t\tnot currently work on AMD cards.\n",
        "#     - Pixel_Gradient_Difference - Instead of minimizing the difference between the absolute value of\n",
        "# \t\teach pixel in two reference images, compute the pixel to pixel spatial difference in each image\n",
        "# \t\tand then minimize that difference between two images. Allows for large color shifts, but maintains\n",
        "# \t\tthe structure of the image.\n",
        "# \n",
        "# Choose from: ['mae', 'mse', 'logcosh', 'smooth_loss', 'l_inf_norm', 'ssim', 'ms_ssim', 'gmsd',\n",
        "# 'pixel_gradient_diff']\n",
        "# [Default: ssim]\n",
        "loss_function = ssim\n",
        "\n",
        "# The loss function to use when learning a mask.\n",
        "#     - MAE - Mean absolute error will guide reconstructions of each pixel towards its median value in\n",
        "# \t\tthe training dataset. Robust to outliers but as a median, it can potentially ignore some\n",
        "# \t\tinfrequent image types in the dataset.\n",
        "#     - MSE - Mean squared error will guide reconstructions of each pixel towards its average value in\n",
        "# \t\tthe training dataset. As an average, it will be susceptible to outliers and typically produces\n",
        "# \t\tslightly blurrier results.\n",
        "# \n",
        "# Choose from: ['mae', 'mse']\n",
        "# [Default: mse]\n",
        "mask_loss_function = mse\n",
        "\n",
        "# The amount of L2 Regularization to apply as a penalty to Structural Similarity loss functions.\n",
        "# \n",
        "# NB: You should only adjust this if you know what you are doing!\n",
        "# \n",
        "# L2 regularization applies a penalty term to the given Loss function. This penalty will only be\n",
        "# applied if SSIM, MS-SSIM or GMSD is selected for the main loss function, otherwise it is ignored.\n",
        "# \n",
        "# The value given here is as a percentage weight of the main loss function. For example:\n",
        "#     - 100 - Will give equal weighting to the main loss and the penalty function.\n",
        "#     - 25 - Will give the penalty function 1/4 of the weight of the main loss function.\n",
        "#     - 400 - Will give the penalty function 4x as much importance as the main loss function.\n",
        "#     - 0 - Disables L2 Regularization altogether.\n",
        "# \n",
        "# Select an integer between 0 and 400\n",
        "# [Default: 100]\n",
        "l2_reg_term = 100\n",
        "\n",
        "# The amount of priority to give to the eyes.\n",
        "# \n",
        "# The value given here is as a multiplier of the main loss score. For example:\n",
        "#     - 1 - The eyes will receive the same priority as the rest of the face.\n",
        "#     - 10 - The eyes will be given a score 10 times higher than the rest of the face.\n",
        "# \n",
        "# NB: Penalized Mask Loss must be enable to use this option.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Select an integer between 1 and 40\n",
        "# [Default: 3]\n",
        "eye_multiplier = 3\n",
        "\n",
        "# The amount of priority to give to the mouth.\n",
        "# \n",
        "# The value given here is as a multiplier of the main loss score. For Example:\n",
        "#     - 1 - The mouth will receive the same priority as the rest of the face.\n",
        "#     - 10 - The mouth will be given a score 10 times higher than the rest of the face.\n",
        "# \n",
        "# NB: Penalized Mask Loss must be enable to use this option.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Select an integer between 1 and 40\n",
        "# [Default: 2]\n",
        "mouth_multiplier = 2\n",
        "\n",
        "# Image loss function is weighted by mask presence. For areas of the image without the facial mask,\n",
        "# reconstruction errors will be ignored while the masked face area is prioritized. May increase\n",
        "# overall quality by focusing attention on the core face area.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "penalized_mask_loss = True\n",
        "\n",
        "# The mask to be used for training. If you have selected 'Learn Mask' or 'Penalized Mask Loss' you\n",
        "# must select a value other than 'none'. The required mask should have been selected as part of the\n",
        "# Extract process. If it does not exist in the alignments file then it will be generated prior to\n",
        "# training commencing.\n",
        "#     - none: Don't use a mask.\n",
        "#     - bisenet-fp-face: Relatively lightweight NN based mask that provides more refined control over\n",
        "# \t\tthe area to be masked (configurable in mask settings). Use this version of bisenet-fp if your\n",
        "# \t\tmodel is trained with 'face' or 'legacy' centering.\n",
        "#     - bisenet-fp-head: Relatively lightweight NN based mask that provides more refined control over\n",
        "# \t\tthe area to be masked (configurable in mask settings). Use this version of bisenet-fp if your\n",
        "# \t\tmodel is trained with 'head' centering.\n",
        "#     - components: Mask designed to provide facial segmentation based on the positioning of landmark\n",
        "# \t\tlocations. A convex hull is constructed around the exterior of the landmarks to create a mask.\n",
        "#     - extended: Mask designed to provide facial segmentation based on the positioning of landmark\n",
        "# \t\tlocations. A convex hull is constructed around the exterior of the landmarks and the mask is\n",
        "# \t\textended upwards onto the forehead.\n",
        "#     - vgg-clear: Mask designed to provide smart segmentation of mostly frontal faces clear of\n",
        "# \t\tobstructions. Profile faces and obstructions may result in sub-par performance.\n",
        "#     - vgg-obstructed: Mask designed to provide smart segmentation of mostly frontal faces. The mask\n",
        "# \t\tmodel has been specifically trained to recognize some facial obstructions (hands and eyeglasses).\n",
        "# \t\tProfile faces may result in sub-par performance.\n",
        "#     - unet-dfl: Mask designed to provide smart segmentation of mostly frontal faces. The mask model\n",
        "# \t\thas been trained by community members and will need testing for further description. Profile faces\n",
        "# \t\tmay result in sub-par performance.\n",
        "# \n",
        "# Choose from: ['none', 'bisenet-fp_face', 'bisenet-fp_head', 'components', 'extended', 'unet-dfl',\n",
        "# 'vgg-clear', 'vgg-obstructed']\n",
        "# [Default: extended]\n",
        "mask_type = extended\n",
        "\n",
        "# Apply gaussian blur to the mask input. This has the effect of smoothing the edges of the mask, which\n",
        "# can help with poorly calculated masks and give less of a hard edge to the predicted mask. The size\n",
        "# is in pixels (calculated from a 128px mask). Set to 0 to not apply gaussian blur. This value should\n",
        "# be odd, if an even number is passed in then it will be rounded to the next odd number.\n",
        "# \n",
        "# Select an integer between 0 and 9\n",
        "# [Default: 3]\n",
        "mask_blur_kernel = 3\n",
        "\n",
        "# Sets pixels that are near white to white and near black to black. Set to 0 for off.\n",
        "# \n",
        "# Select an integer between 0 and 50\n",
        "# [Default: 4]\n",
        "mask_threshold = 4\n",
        "\n",
        "# Dedicate a portion of the model to learning how to duplicate the input mask. Increases VRAM usage in\n",
        "# exchange for learning a quick ability to try to replicate more complex mask models.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "learn_mask = False\n",
        "\n",
        "[model.dfaker]\n",
        "# DFAKER MODEL (ADAPTED FROM HTTPS://GITHUB.COM/DFAKER/DF)\n",
        "\n",
        "# Resolution (in pixels) of the output image to generate on.\n",
        "# BE AWARE Larger resolution will dramatically increase VRAM requirements.\n",
        "# Must be 128 or 256.\n",
        "# \n",
        "# Select an integer between 128 and 256\n",
        "# [Default: 128]\n",
        "output_size = 128\n",
        "\n",
        "[model.dfl_h128]\n",
        "# DFL H128 MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)\n",
        "\n",
        "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
        "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "lowmem = False\n",
        "\n",
        "[model.dfl_sae]\n",
        "# DFL SAE MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)\n",
        "\n",
        "# Resolution (in pixels) of the input image to train on.\n",
        "# BE AWARE Larger resolution will dramatically increase VRAM requirements.\n",
        "# \n",
        "# Must be divisible by 16.\n",
        "# \n",
        "# Select an integer between 64 and 256\n",
        "# [Default: 128]\n",
        "input_size = 128\n",
        "\n",
        "# Controls gradient clipping of the optimizer. Can prevent model corruption at the expense of VRAM.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "clipnorm = True\n",
        "\n",
        "# Model architecture:\n",
        "#     - 'df': Keeps the faces more natural.\n",
        "#     - 'liae': Can help fix overly different face shapes.\n",
        "# \n",
        "# Choose from: ['df', 'liae']\n",
        "# [Default: df]\n",
        "architecture = df\n",
        "\n",
        "# Face information is stored in AutoEncoder dimensions. If there are not enough dimensions then\n",
        "# certain facial features may not be recognized.\n",
        "# Higher number of dimensions are better, but require more VRAM.\n",
        "# Set to 0 to use the architecture defaults (256 for liae, 512 for df).\n",
        "# \n",
        "# Select an integer between 0 and 1024\n",
        "# [Default: 0]\n",
        "autoencoder_dims = 0\n",
        "\n",
        "# Encoder dimensions per channel. Higher number of encoder dimensions will help the model to recognize\n",
        "# more facial features, but will require more VRAM.\n",
        "# \n",
        "# Select an integer between 21 and 85\n",
        "# [Default: 42]\n",
        "encoder_dims = 42\n",
        "\n",
        "# Decoder dimensions per channel. Higher number of decoder dimensions will help the model to improve\n",
        "# details, but will require more VRAM.\n",
        "# \n",
        "# Select an integer between 10 and 85\n",
        "# [Default: 21]\n",
        "decoder_dims = 21\n",
        "\n",
        "# Multiscale decoder can help to obtain better details.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "multiscale_decoder = False\n",
        "\n",
        "[model.dlight]\n",
        "# A LIGHTWEIGHT, HIGH RESOLUTION DFAKER VARIANT (ADAPTED FROM HTTPS://GITHUB.COM/DFAKER/DF)\n",
        "\n",
        "# Higher settings will allow learning more features such as tatoos, piercing and wrinkles.\n",
        "# Strongly affects VRAM usage.\n",
        "# \n",
        "# Choose from: ['lowmem', 'fair', 'best']\n",
        "# [Default: best]\n",
        "features = best\n",
        "\n",
        "# Defines detail fidelity. Lower setting can appear 'rugged' while 'good' might take a longer time to\n",
        "# train.\n",
        "# Affects VRAM usage.\n",
        "# \n",
        "# Choose from: ['fast', 'good']\n",
        "# [Default: good]\n",
        "details = good\n",
        "\n",
        "# Output image resolution (in pixels).\n",
        "# Be aware that larger resolution will increase VRAM requirements.\n",
        "# NB: Must be either 128, 256, or 384.\n",
        "# \n",
        "# Select an integer between 128 and 384\n",
        "# [Default: 256]\n",
        "output_size = 256\n",
        "\n",
        "[model.original]\n",
        "# ORIGINAL FACESWAP MODEL.\n",
        "\n",
        "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
        "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "lowmem = False\n",
        "\n",
        "[model.phaze_a]\n",
        "# PHAZE-A MODEL BY TORZDF, WITH THANKS TO BIRBFAKES.\n",
        "# ALLOWS FOR THE EXPERIMENTATION OF VARIOUS STANDARD NETWORKS AS THE ENCODER AND TAKES INSPIRATION\n",
        "# FROM NVIDIA'S STYLEGAN FOR THE DECODER. IT IS HIGHLY RECOMMENDED TO RESEARCH TO UNDERSTAND THE\n",
        "# PARAMETERS BETTER.\n",
        "\n",
        "# Resolution (in pixels) of the output image to generate.\n",
        "# BE AWARE Larger resolution will dramatically increase VRAM requirements.\n",
        "# \n",
        "# Select an integer between 64 and 1024\n",
        "# [Default: 128]\n",
        "output_size = 128\n",
        "\n",
        "# Whether to create a shared fully connected layer. This layer will have the same structure as the\n",
        "# fully connected layers used for each side of the model. A shared fully connected layer looks for\n",
        "# patterns that are common to both sides. NB: Enabling this option only makes sense if 'split fc' is\n",
        "# selected.\n",
        "#     - none - Do not create a Fully Connected layer for shared data. (Original method)\n",
        "#     - full - Create an exclusive Fully Connected layer for shared data. (IAE method)\n",
        "#     - half - Use the 'fc_a' layer for shared data. This saves VRAM by re-using the 'A' side's fully\n",
        "# \t\tconnected model for the shared data. However, this will lead to an 'unbalanced' model and can lead\n",
        "# \t\tto more identity bleed (DFL method)\n",
        "# \n",
        "# Choose from: ['none', 'full', 'half']\n",
        "# [Default: none]\n",
        "shared_fc = none\n",
        "\n",
        "# Whether to enable the G-Block. If enabled, this will create a shared fully connected layer\n",
        "# (configurable in the 'G-Block hidden layers' section) to look for patterns in the combined data,\n",
        "# before feeding a block prior to the decoder for merging this shared and combined data.\n",
        "#     - True - Use the G-Block in the Decoder. A combined fully connected layer will be created to\n",
        "# \t\tfeed this block which can be configured below.\n",
        "#     - False - Don't use the G-Block in the decoder. No combined fully connected layer will be\n",
        "# \t\tcreated.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "enable_gblock = True\n",
        "\n",
        "# Whether to use a single shared Fully Connected layer or separate Fully Connected layers for each\n",
        "# side.\n",
        "#     - True - Use separate Fully Connected layers for Face A and Face B. This is more similar to the\n",
        "# \t\t'IAE' style of model.\n",
        "#     - False - Use combined Fully Connected layers for both sides. This is more similar to the\n",
        "# \t\toriginal Faceswap architecture.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "split_fc = True\n",
        "\n",
        "# If the G-Block is enabled, Whether to use a single G-Block shared between both sides, or whether to\n",
        "# have a separate G-Block (one for each side). NB: The Fully Connected layer that feeds the G-Block\n",
        "# will always be shared.\n",
        "#     - True - Use separate G-Blocks for Face A and Face B.\n",
        "#     - False - Use a combined G-Block layers for both sides.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "split_gblock = False\n",
        "\n",
        "# Whether to use a single decoder or split decoders.\n",
        "#     - True - Use a separate decoder for Face A and Face B. This is more similar to the original\n",
        "# \t\tFaceswap architecture.\n",
        "#     - False - Use a combined Decoder. This is more similar to 'IAE' style architecture.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "split_decoders = False\n",
        "\n",
        "# The encoder architecture to use. See the relevant config sections for specific architecture\n",
        "# tweaking.\n",
        "# NB: For keras based pre-built models, the global initializers and padding options will be ignored\n",
        "# for the selected encoder.\n",
        "#     - densenet: (32px -224px). Ref: Densely Connected Convolutional Networks (2016):\n",
        "# \t\thttps://arxiv.org/abs/1608.06993?source=post_page\n",
        "#     - efficientnet: [Tensorflow 2.3+ only] EfficientNet has numerous variants (B0 - B8) that\n",
        "# \t\tincreases the model width, depth and dimensional space at each step. The minimum input resolution\n",
        "# \t\tis 32px for all variants. The maximum input resolution for each variant is: b0: 224px, b1: 240px,\n",
        "# \t\tb2: 260px, b3: 300px, b4: 380px, b5: 456px, b6: 528px, b7 600px. Ref: Rethinking Model Scaling for\n",
        "# \t\tConvolutional Neural Networks (2020): https://arxiv.org/abs/1905.11946\n",
        "#     - efficientnet_v2: [Tensorflow 2.8+ only] EfficientNetV2 is the follow up to efficientnet. It\n",
        "# \t\thas numerous variants (B0 - B3 and Small, Medium and Large) that increases the model width, depth\n",
        "# \t\tand dimensional space at each step. The minimum input resolution is 32px for all variants. The\n",
        "# \t\tmaximum input resolution for each variant is: b0: 224px, b1: 240px, b2: 260px, b3: 300px, s:\n",
        "# \t\t384px, m: 480px, l: 480px. Ref: EfficientNetV2: Smaller Models and Faster Training (2021):\n",
        "# \t\thttps://arxiv.org/abs/2104.00298\n",
        "#     - fs_original: (32px - 160px). A configurable variant of the original facewap encoder. ImageNet\n",
        "# \t\tweights cannot be loaded for this model. Additional parameters can be configured with the 'fs_enc'\n",
        "# \t\toptions. A version of this encoder is used in the following models: Original, Original (lowmem),\n",
        "# \t\tDfaker, DFL-H128, DFL-SAE, IAE, Lightweight.\n",
        "#     - inception_resnet_v2: (75px - 299px). Ref: Inception-ResNet and the Impact of Residual\n",
        "# \t\tConnections on Learning (2016): https://arxiv.org/abs/1602.07261\n",
        "#     - inceptionV3: (75px - 299px). Ref: Rethinking the Inception Architecture for Computer Vision\n",
        "# \t\t(2015): https://arxiv.org/abs/1512.00567\n",
        "#     - mobilenet: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'\n",
        "# \t\toptions. Ref: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n",
        "# \t\t(2017): https://arxiv.org/abs/1704.04861\n",
        "#     - mobilenet_v2: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'\n",
        "# \t\toptions. Ref: MobileNetV2: Inverted Residuals and Linear Bottlenecks (2018):\n",
        "# \t\thttps://arxiv.org/abs/1801.04381\n",
        "#     - mobilenet_v3: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'\n",
        "# \t\toptions. Ref: Searching for MobileNetV3 (2019): https://arxiv.org/pdf/1905.02244.pdf\n",
        "#     - nasnet: (32px - 331px (large) or 224px (mobile)). Ref: Learning Transferable Architectures for\n",
        "# \t\tScalable Image Recognition (2017): https://arxiv.org/abs/1707.07012\n",
        "#     - resnet: (32px - 224px). Deep Residual Learning for Image Recognition (2015):\n",
        "# \t\thttps://arxiv.org/abs/1512.03385\n",
        "#     - vgg: (32px - 224px). Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
        "# \t\t(2014): https://arxiv.org/abs/1409.1556\n",
        "#     - xception: (71px - 229px). Ref: Deep Learning with Depthwise Separable Convolutions (2017):\n",
        "# \t\thttps://arxiv.org/abs/1409.1556.\n",
        "# \n",
        "# \n",
        "# Choose from: ['densenet121', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1',\n",
        "# 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6',\n",
        "# 'efficientnet_b7', 'efficientnet_v2_b0', 'efficientnet_v2_b1', 'efficientnet_v2_b2',\n",
        "# 'efficientnet_v2_b3', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'fs_original',\n",
        "# 'inception_resnet_v2', 'inception_v3', 'mobilenet', 'mobilenet_v2', 'mobilenet_v3_large',\n",
        "# 'mobilenet_v3_small', 'nasnet_large', 'nasnet_mobile', 'resnet101', 'resnet101_v2', 'resnet152',\n",
        "# 'resnet152_v2', 'resnet50', 'resnet50_v2', 'vgg16', 'vgg19', 'xception']\n",
        "# [Default: fs_original]\n",
        "enc_architecture = fs_original\n",
        "\n",
        "# Input scaling for the encoder. Some of the encoders have large input sizes, which often are not\n",
        "# helpful for Faceswap. This setting scales the dimensional space that the encoder works in. For\n",
        "# example an encoder with a maximum input size of 224px will be input an image of 112px at 50%%\n",
        "# scaling. See the Architecture tooltip for the minimum and maximum sizes for each encoder.\n",
        "# \n",
        "# Select an integer between 0 and 100\n",
        "# [Default: 40]\n",
        "enc_scaling = 40\n",
        "\n",
        "# Load pre-trained weights trained on ImageNet data. Only available for non-Faceswap encoders (i.e.\n",
        "# those not beginning with 'fs'). NB: If you use the global 'load weights' option and have selected to\n",
        "# load weights from a previous model's 'encoder' or 'keras_encoder' then the weights loaded here will\n",
        "# be replaced by the weights loaded from your saved model.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "enc_load_weights = True\n",
        "\n",
        "# The type of layer to use for the bottleneck.\n",
        "#     - average_pooling: Use a Global Average Pooling 2D layer for the bottleneck.\n",
        "#     - dense: Use a Dense layer for the bottleneck (the traditional Faceswap method). You can set the\n",
        "# \t\tsize of the Dense layer with the 'bottleneck_size' parameter.\n",
        "#     - max_pooling: Use a Global Max Pooling 2D layer for the bottleneck.\n",
        "# \n",
        "# Choose from: ['average_pooling', 'dense', 'max_pooling']\n",
        "# [Default: dense]\n",
        "bottleneck_type = dense\n",
        "\n",
        "# Apply a normalization layer after encoder output and prior to the bottleneck.\n",
        "#     - none - Do not apply a normalization layer\n",
        "#     - instance - Apply Instance Normalization\n",
        "#     - layer - Apply Layer Normalization (Ba et al., 2016)\n",
        "#     - rms - Apply Root Mean Squared Layer Normalization (Zhang et al., 2019). A simplified version\n",
        "# \t\tof Layer Normalization with reduced overhead.\n",
        "# \n",
        "# Choose from: ['none', 'instance', 'layer', 'rms']\n",
        "# [Default: none]\n",
        "bottleneck_norm = none\n",
        "\n",
        "# If using a Dense layer for the bottleneck, then this is the number of nodes to use.\n",
        "# \n",
        "# Select an integer between 128 and 4096\n",
        "# [Default: 1024]\n",
        "bottleneck_size = 1024\n",
        "\n",
        "# Whether to place the bottleneck in the Encoder or to place it with the other hidden layers. Placing\n",
        "# the bottleneck in the encoder means that both sides will share the same bottleneck. Placing it with\n",
        "# the other fully connected layers means that each fully connected layer will each get their own\n",
        "# bottleneck. This may be combined or split depending on your overall architecture configuration\n",
        "# settings.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "bottleneck_in_encoder = True\n",
        "\n",
        "# The number of consecutive Dense (fully connected) layers to include in each side's intermediate\n",
        "# layer.\n",
        "# \n",
        "# Select an integer between 1 and 16\n",
        "# [Default: 1]\n",
        "fc_depth = 1\n",
        "\n",
        "# The number of filters to use for the initial fully connected layer. The number of nodes actually\n",
        "# used is: fc_min_filters x fc_dimensions x fc_dimensions.\n",
        "# NB: This value may be scaled down, depending on output resolution.\n",
        "# \n",
        "# Select an integer between 16 and 5120\n",
        "# [Default: 1024]\n",
        "fc_min_filters = 1024\n",
        "\n",
        "# This is the number of filters to be used in the final reshape layer at the end of the fully\n",
        "# connected layers. The actual number of nodes used for the final fully connected layer is:\n",
        "# fc_min_filters x fc_dimensions x fc_dimensions.\n",
        "# NB: This value may be scaled down, depending on output resolution.\n",
        "# \n",
        "# Select an integer between 128 and 5120\n",
        "# [Default: 1024]\n",
        "fc_max_filters = 1024\n",
        "\n",
        "# The height and width dimension for the final reshape layer at the end of the fully connected layers.\n",
        "# NB: The total number of nodes within the final fully connected layer will be: fc_dimensions x\n",
        "# fc_dimensions x fc_max_filters.\n",
        "# \n",
        "# Select an integer between 3 and 16\n",
        "# [Default: 4]\n",
        "fc_dimensions = 4\n",
        "\n",
        "# The rate that the filters move from the minimum number of filters to the maximum number of filters.\n",
        "# EG:\n",
        "# Negative numbers will change the number of filters quicker at first and slow down each layer.\n",
        "# Positive numbers will change the number of filters slower at first but then speed up each layer.\n",
        "# 0.0 - This will change at a linear rate (i.e. the same number of filters will be changed at each\n",
        "# layer).\n",
        "# \n",
        "# Select a decimal number between -0.99 and 0.99\n",
        "# [Default: -0.5]\n",
        "fc_filter_slope = -0.5\n",
        "\n",
        "# Dropout is a form of regularization that can prevent a model from over-fitting and help to keep\n",
        "# neurons 'alive'. 0.5 will dropout half the connections between each fully connected layer, 0.25 will\n",
        "# dropout a quarter of the connections etc. Set to 0.0 to disable.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Select a decimal number between 0.0 and 0.99\n",
        "# [Default: 0.0]\n",
        "fc_dropout = 0.0\n",
        "\n",
        "# The type of dimensional upsampling to perform at the end of the fully connected layers, if upsamples\n",
        "# > 0. The number of filters used for the upscale layers will be the value given in\n",
        "# 'fc_upsample_filters'.\n",
        "#     - upsample2d - A lightweight and VRAM friendly method. 'quick and dirty' but does not learn any\n",
        "# \t\tparameters\n",
        "#     - subpixel - Sub-pixel upscaler using depth-to-space which may require more VRAM.\n",
        "#     - resize_images - Uses the Keras resize_image function to save about half as much vram as the\n",
        "# \t\theaviest methods.\n",
        "#     - upscale_fast - Developed by Andenixa. Focusses on speed to upscale, but requires more VRAM.\n",
        "#     - upscale_hybrid - Developed by Andenixa. Uses a combination of PixelShuffler and Upsampling2D\n",
        "# \t\tto upscale, saving about 1/3rd of VRAM of the heaviest methods.\n",
        "# \n",
        "# Choose from: ['resize_images', 'subpixel', 'upscale_fast', 'upscale_hybrid', 'upsample2d']\n",
        "# [Default: upsample2d]\n",
        "fc_upsampler = upsample2d\n",
        "\n",
        "# Some upsampling can occur within the Fully Connected layers rather than in the Decoder to increase\n",
        "# the dimensional space. Set how many upscale layers should occur within the Fully Connected layers.\n",
        "# \n",
        "# Select an integer between 0 and 4\n",
        "# [Default: 1]\n",
        "fc_upsamples = 1\n",
        "\n",
        "# If you have selected an upsampler which requires filters (i.e. any upsampler with the exception of\n",
        "# Upsampling2D), then this is the number of filters to be used for the upsamplers within the fully\n",
        "# connected layers,  NB: This value may be scaled down, depending on output resolution. Also note,\n",
        "# that this figure will dictate the number of filters used for the G-Block, if selected.\n",
        "# \n",
        "# Select an integer between 128 and 5120\n",
        "# [Default: 512]\n",
        "fc_upsample_filters = 512\n",
        "\n",
        "# The number of consecutive Dense (fully connected) layers to include in the G-Block shared layer.\n",
        "# \n",
        "# Select an integer between 1 and 16\n",
        "# [Default: 3]\n",
        "fc_gblock_depth = 3\n",
        "\n",
        "# The number of nodes to use for the initial G-Block shared fully connected layer.\n",
        "# \n",
        "# Select an integer between 128 and 5120\n",
        "# [Default: 512]\n",
        "fc_gblock_min_nodes = 512\n",
        "\n",
        "# The number of nodes to use for the final G-Block shared fully connected layer.\n",
        "# \n",
        "# Select an integer between 128 and 5120\n",
        "# [Default: 512]\n",
        "fc_gblock_max_nodes = 512\n",
        "\n",
        "# The rate that the filters move from the minimum number of filters to the maximum number of filters\n",
        "# for the G-Block shared layers. EG:\n",
        "# Negative numbers will change the number of filters quicker at first and slow down each layer.\n",
        "# Positive numbers will change the number of filters slower at first but then speed up each layer.\n",
        "# 0.0 - This will change at a linear rate (i.e. the same number of filters will be changed at each\n",
        "# layer).\n",
        "# \n",
        "# Select a decimal number between -0.99 and 0.99\n",
        "# [Default: -0.5]\n",
        "fc_gblock_filter_slope = -0.5\n",
        "\n",
        "# Dropout is a regularization technique that can prevent a model from over-fitting and help to keep\n",
        "# neurons 'alive'. 0.5 will dropout half the connections between each fully connected layer, 0.25 will\n",
        "# dropout a quarter of the connections etc. Set to 0.0 to disable.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Select a decimal number between 0.0 and 0.99\n",
        "# [Default: 0.0]\n",
        "fc_gblock_dropout = 0.0\n",
        "\n",
        "# The method to use for the upscales within the decoder. Images are upscaled multiple times within the\n",
        "# decoder as the network learns to reconstruct the face.\n",
        "#     - subpixel - Sub-pixel upscaler using depth-to-space which requires more VRAM.\n",
        "#     - resize_images - Uses the Keras resize_image function to save about half as much vram as the\n",
        "# \t\theaviest methods.\n",
        "#     - upscale_fast - Developed by Andenixa. Focusses on speed to upscale, but requires more VRAM.\n",
        "#     - upscale_hybrid - Developed by Andenixa. Uses a combination of PixelShuffler and Upsampling2D\n",
        "# \t\tto upscale, saving about 1/3rd of VRAM of the heaviest methods.\n",
        "# \n",
        "# Choose from: ['subpixel', 'resize_images', 'upscale_fast', 'upscale_hybrid']\n",
        "# [Default: subpixel]\n",
        "dec_upscale_method = subpixel\n",
        "\n",
        "# Normalization to apply to apply after each upscale.\n",
        "#     - none - Do not apply a normalization layer\n",
        "#     - batch - Apply Batch Normalization\n",
        "#     - group - Apply Group Normalization\n",
        "#     - instance - Apply Instance Normalization\n",
        "#     - layer - Apply Layer Normalization (Ba et al., 2016)\n",
        "#     - rms - Apply Root Mean Squared Layer Normalization (Zhang et al., 2019). A simplified version\n",
        "# \t\tof Layer Normalization with reduced overhead.\n",
        "# \n",
        "# Choose from: ['none', 'batch', 'group', 'instance', 'layer', 'rms']\n",
        "# [Default: none]\n",
        "dec_norm = none\n",
        "\n",
        "# The minimum number of filters to use in decoder upscalers (i.e. the number of filters to use for the\n",
        "# final upscale layer).\n",
        "# \n",
        "# Select an integer between 64 and 512\n",
        "# [Default: 64]\n",
        "dec_min_filters = 64\n",
        "\n",
        "# The maximum number of filters to use in decoder upscalers (i.e. the number of filters to use for the\n",
        "# first upscale layer).\n",
        "# \n",
        "# Select an integer between 256 and 5120\n",
        "# [Default: 512]\n",
        "dec_max_filters = 512\n",
        "\n",
        "# The rate that the filters reduce at each upscale layer. EG:\n",
        "# Negative numbers will drop the number of filters quicker at first and slow down each upscale.\n",
        "# Positive numbers will drop the number of filters slower at first but then speed up each upscale.\n",
        "# 0.0 - This will reduce at a linear rate (i.e. the same number of filters will be reduced at each\n",
        "# upscale).\n",
        "# \n",
        "# Select a decimal number between -0.99 and 0.99\n",
        "# [Default: -0.45]\n",
        "dec_filter_slope = -0.45\n",
        "\n",
        "# The number of Residual Blocks to apply to each upscale layer. Set to 0 to disable residual blocks\n",
        "# entirely.\n",
        "# \n",
        "# Select an integer between 0 and 8\n",
        "# [Default: 1]\n",
        "dec_res_blocks = 1\n",
        "\n",
        "# The kernel size to apply to the final Convolution layer.\n",
        "# \n",
        "# Select an integer between 1 and 9\n",
        "# [Default: 5]\n",
        "dec_output_kernel = 5\n",
        "\n",
        "# Gaussian Noise acts as a regularization technique for preventing overfitting of data.\n",
        "#     - True - Apply a Gaussian Noise layer to each upscale.\n",
        "#     - False - Don't apply a Gaussian Noise layer to each upscale.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "dec_gaussian = True\n",
        "\n",
        "# If Residual blocks have been enabled, enabling this option will not apply a Residual block to the\n",
        "# final upscaler.\n",
        "#     - True - Don't apply a Residual block to the final upscale.\n",
        "#     - False - Apply a Residual block to all upscale layers.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "dec_skip_last_residual = True\n",
        "\n",
        "# If the command line option 'freeze-weights' is enabled, then the layers indicated here will be\n",
        "# frozen the next time the model starts up. NB: Not all architectures contain all of the layers listed\n",
        "# here, so any layers marked for freezing that are not within your chosen architecture will be\n",
        "# ignored. EG:\n",
        "#  If 'split fc' has been selected, then 'fc_a' and 'fc_b' are available for freezing. If it has not\n",
        "# been selected then 'fc_both' is available for freezing.\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# If selecting multiple options then each option should be separated by a space or a comma (e.g.\n",
        "# item1, item2, item3)\n",
        "# \n",
        "# Choose from: ['encoder', 'keras_encoder', 'fc_a', 'fc_b', 'fc_both', 'fc_shared', 'fc_gblock',\n",
        "# 'g_block_a', 'g_block_b', 'g_block_both', 'decoder_a', 'decoder_b', 'decoder_both']\n",
        "# [Default: keras_encoder]\n",
        "freeze_layers = keras_encoder\n",
        "\n",
        "# If the command line option 'load-weights' is populated, then the layers indicated here will be\n",
        "# loaded from the given weights file if starting a new model. NB Not all architectures contain all of\n",
        "# the layers listed here, so any layers marked for loading that are not within your chosen\n",
        "# architecture will be ignored. EG:\n",
        "#  If 'split fc' has been selected, then 'fc_a' and 'fc_b' are available for loading. If it has not\n",
        "# been selected then 'fc_both' is available for loading.\n",
        "# \n",
        "# If selecting multiple options then each option should be separated by a space or a comma (e.g.\n",
        "# item1, item2, item3)\n",
        "# \n",
        "# Choose from: ['encoder', 'fc_a', 'fc_b', 'fc_both', 'fc_shared', 'fc_gblock', 'g_block_a',\n",
        "# 'g_block_b', 'g_block_both', 'decoder_a', 'decoder_b', 'decoder_both']\n",
        "# [Default: encoder]\n",
        "load_layers = encoder\n",
        "\n",
        "# Faceswap Encoder only: The number of convolutions to perform within the encoder.\n",
        "# \n",
        "# Select an integer between 2 and 10\n",
        "# [Default: 4]\n",
        "fs_original_depth = 4\n",
        "\n",
        "# Faceswap Encoder only: The minumum number of filters to use for encoder convolutions. (i.e. the\n",
        "# number of filters to use for the first encoder layer).\n",
        "# \n",
        "# Select an integer between 64 and 2048\n",
        "# [Default: 128]\n",
        "fs_original_min_filters = 128\n",
        "\n",
        "# Faceswap Encoder only: The maximum number of filters to use for encoder convolutions. (i.e. the\n",
        "# number of filters to use for the final encoder layer).\n",
        "# \n",
        "# Select an integer between 256 and 8192\n",
        "# [Default: 1024]\n",
        "fs_original_max_filters = 1024\n",
        "\n",
        "# The width multiplier for mobilenet encoders. Controls the width of the network. Values less than 1.0\n",
        "# proportionally decrease the number of filters within each layer. Values greater than 1.0\n",
        "# proportionally increase the number of filters within each layer. 1.0 is the default number of layers\n",
        "# used within the paper.\n",
        "# NB: This option is ignored for any non-mobilenet encoders.\n",
        "# NB: If loading ImageNet weights, then for MobilenetV1 only values of '0.25', '0.5', '0.75' or '1.0\n",
        "# can be selected. For MobilenetV2 only values of '0.35', '0.50', '0.75', '1.0', '1.3' or '1.4' can be\n",
        "# selected. For mobilenet_v3 only values of '0.75' or '1.0' can be selected\n",
        "# \n",
        "# Select a decimal number between 0.1 and 2.0\n",
        "# [Default: 1.0]\n",
        "mobilenet_width = 1.0\n",
        "\n",
        "# The depth multiplier for MobilenetV1 encoder. This is the depth multiplier for depthwise convolution\n",
        "# (known as the resolution multiplier within the original paper).\n",
        "# NB: This option is only used for MobilenetV1 and is ignored for all other encoders.\n",
        "# NB: If loading ImageNet weights, this must be set to 1.\n",
        "# \n",
        "# Select an integer between 1 and 10\n",
        "# [Default: 1]\n",
        "mobilenet_depth = 1\n",
        "\n",
        "# The dropout rate for MobilenetV1 encoder.\n",
        "# NB: This option is only used for MobilenetV1 and is ignored for all other encoders.\n",
        "# \n",
        "# Select a decimal number between 0.001 and 2.0\n",
        "# [Default: 0.001]\n",
        "mobilenet_dropout = 0.001\n",
        "\n",
        "# Use a minimilist version of MobilenetV3.\n",
        "# In addition to large and small models MobilenetV3 also contains so-called minimalistic models, these\n",
        "# models have the same per-layer dimensions characteristic as MobilenetV3 however, they don't utilize\n",
        "# any of the advanced blocks (squeeze-and-excite units, hard-swish, and 5x5 convolutions). While these\n",
        "# models are less efficient on CPU, they are much more performant on GPU/DSP.\n",
        "# NB: This option is only used for MobilenetV3 and is ignored for all other encoders.\n",
        "# \n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "mobilenet_minimalistic = False\n",
        "\n",
        "[model.realface]\n",
        "# AN EXTRA DETAILED VARIANT OF ORIGINAL MODEL.\n",
        "# INCORPORATES IDEAS FROM BRYANLYON AND INSPIRATION FROM THE VILLAIN MODEL.\n",
        "# REQUIRES ABOUT 6GB-8GB OF VRAM (BATCHSIZE 8-16).\n",
        "# \n",
        "\n",
        "# Resolution (in pixels) of the input image to train on.\n",
        "# BE AWARE Larger resolution will dramatically increase VRAM requirements.\n",
        "# Higher resolutions may increase prediction accuracy, but does not effect the resulting output size.\n",
        "# Must be between 64 and 128 and be divisible by 16.\n",
        "# \n",
        "# Select an integer between 64 and 128\n",
        "# [Default: 64]\n",
        "input_size = 64\n",
        "\n",
        "# Output image resolution (in pixels).\n",
        "# Be aware that larger resolution will increase VRAM requirements.\n",
        "# NB: Must be between 64 and 256 and be divisible by 16.\n",
        "# \n",
        "# Select an integer between 64 and 256\n",
        "# [Default: 128]\n",
        "output_size = 128\n",
        "\n",
        "# Number of nodes for decoder. Might affect your model's ability to learn in general.\n",
        "# Note that: Lower values will affect the ability to predict details.\n",
        "# \n",
        "# Select an integer between 768 and 2048\n",
        "# [Default: 1536]\n",
        "dense_nodes = 1536\n",
        "\n",
        "# Encoder Convolution Layer Complexity. sensible ranges: 128 to 150.\n",
        "# \n",
        "# Select an integer between 96 and 160\n",
        "# [Default: 128]\n",
        "complexity_encoder = 128\n",
        "\n",
        "# Decoder Complexity.\n",
        "# \n",
        "# Select an integer between 512 and 544\n",
        "# [Default: 512]\n",
        "complexity_decoder = 512\n",
        "\n",
        "[model.unbalanced]\n",
        "# AN UNBALANCED MODEL WITH ADJUSTABLE INPUT SIZE OPTIONS.\n",
        "# THIS IS AN UNBALANCED MODEL SO B>A SWAPS MAY NOT WORK WELL\n",
        "# \n",
        "\n",
        "# Resolution (in pixels) of the image to train on.\n",
        "# BE AWARE Larger resolution will dramatically increaseVRAM requirements.\n",
        "# Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).\n",
        "# NB: Your faceset must be at least 1.6x larger than your required input size.\n",
        "# (e.g. 160 is the maximum input size for a 256x256 faceset).\n",
        "# \n",
        "# Select an integer between 64 and 512\n",
        "# [Default: 128]\n",
        "input_size = 128\n",
        "\n",
        "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
        "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
        "# NB: lowmem will override cutom nodes and complexity settings.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "lowmem = False\n",
        "\n",
        "# Controls gradient clipping of the optimizer. Can prevent model corruption at the expense of VRAM.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "clipnorm = True\n",
        "\n",
        "# Number of nodes for decoder. Don't change this unless you know what you are doing!\n",
        "# \n",
        "# Select an integer between 512 and 4096\n",
        "# [Default: 1024]\n",
        "nodes = 1024\n",
        "\n",
        "# Encoder Convolution Layer Complexity. sensible ranges: 128 to 160.\n",
        "# \n",
        "# Select an integer between 64 and 1024\n",
        "# [Default: 128]\n",
        "complexity_encoder = 128\n",
        "\n",
        "# Decoder A Complexity.\n",
        "# \n",
        "# Select an integer between 64 and 1024\n",
        "# [Default: 384]\n",
        "complexity_decoder_a = 384\n",
        "\n",
        "# Decoder B Complexity.\n",
        "# \n",
        "# Select an integer between 64 and 1024\n",
        "# [Default: 512]\n",
        "complexity_decoder_b = 512\n",
        "\n",
        "[model.villain]\n",
        "# A HIGHER RESOLUTION VERSION OF THE ORIGINAL MODEL BY VILLAINGUY.\n",
        "# EXTREMELY VRAM HEAVY. DON'T TRY TO RUN THIS IF YOU HAVE A SMALL GPU.\n",
        "# \n",
        "\n",
        "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
        "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "lowmem = False\n",
        "\n",
        "[trainer.original]\n",
        "# ORIGINAL TRAINER OPTIONS.\n",
        "# WARNING: THE DEFAULTS FOR AUGMENTATION WILL BE FINE FOR 99.9% OF USE CASES. ONLY CHANGE THEM IF YOU\n",
        "# ABSOLUTELY KNOW WHAT YOU ARE DOING!\n",
        "\n",
        "# Number of sample faces to display for each side in the preview when training.\n",
        "# \n",
        "# Select an integer between 2 and 16\n",
        "# [Default: 14]\n",
        "preview_images = 14\n",
        "\n",
        "# Percentage amount to randomly zoom each training image in and out.\n",
        "# \n",
        "# Select an integer between 0 and 25\n",
        "# [Default: 5]\n",
        "zoom_amount = 5\n",
        "\n",
        "# Percentage amount to randomly rotate each training image.\n",
        "# \n",
        "# Select an integer between 0 and 25\n",
        "# [Default: 10]\n",
        "rotation_range = 10\n",
        "\n",
        "# Percentage amount to randomly shift each training image horizontally and vertically.\n",
        "# \n",
        "# Select an integer between 0 and 25\n",
        "# [Default: 5]\n",
        "shift_range = 5\n",
        "\n",
        "# Percentage chance to randomly flip each training image horizontally.\n",
        "# NB: This is ignored if the 'no-flip' option is enabled\n",
        "# \n",
        "# Select an integer between 0 and 75\n",
        "# [Default: 50]\n",
        "flip_chance = 50\n",
        "\n",
        "# Percentage amount to randomly alter the lightness of each training image.\n",
        "# NB: This is ignored if the 'no-flip' option is enabled\n",
        "# \n",
        "# Select an integer between 0 and 75\n",
        "# [Default: 30]\n",
        "color_lightness = 30\n",
        "\n",
        "# Percentage amount to randomly alter the 'a' and 'b' colors of the L*a*b* color space of each\n",
        "# training image.\n",
        "# NB: This is ignored if the 'no-flip' option is enabled\n",
        "# \n",
        "# Select an integer between 0 and 50\n",
        "# [Default: 8]\n",
        "color_ab = 8\n",
        "\n",
        "# Percentage chance to perform Contrast Limited Adaptive Histogram Equalization on each training\n",
        "# image.\n",
        "# NB: This is ignored if the 'no-augment-color' option is enabled\n",
        "# \n",
        "# This option can be updated for existing models.\n",
        "# \n",
        "# Select an integer between 0 and 75\n",
        "# [Default: 50]\n",
        "color_clahe_chance = 50\n",
        "\n",
        "# The grid size dictates how much Contrast Limited Adaptive Histogram Equalization is performed on any\n",
        "# training image selected for clahe. Contrast will be applied randomly with a gridsize of 0 up to the\n",
        "# maximum. This value is a multiplier calculated from the training image size.\n",
        "# NB: This is ignored if the 'no-augment-color' option is enabled\n",
        "# \n",
        "# Select an integer between 1 and 8\n",
        "# [Default: 4]\n",
        "color_clahe_max_size = 4\n",
        "\"\"\"\n",
        "\n",
        "with open(\"faceswap/config/train.ini\", \"w\") as text_file:\n",
        "    text_file.write(config)"
      ],
      "metadata": {
        "id": "mbCQEbxhMz0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# convert.ini"
      ],
      "metadata": {
        "id": "T2CcguuPNmdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = \"\"\"\n",
        "[color.color_transfer]\n",
        "# OPTIONS FOR TRANSFERING THE COLOR DISTRIBUTION FROM THE SOURCE TO THE TARGET IMAGE USING THE MEAN\n",
        "# AND STANDARD DEVIATIONS OF THE L*A*B* COLOR SPACE.\n",
        "# THIS IMPLEMENTATION IS (LOOSELY) BASED ON THE 'COLOR TRANSFER BETWEEN IMAGES' PAPER BY REINHARD ET\n",
        "# AL., 2001. MATCHING THE HISTOGRAMS BETWEEN THE SOURCE AND DESTINATION FACES.\n",
        "\n",
        "# Should components of L*a*b* image be scaled by np.clip before converting back to BGR color space?\n",
        "# If False then components will be min-max scaled appropriately.\n",
        "# Clipping will keep target image brightness truer to the input.\n",
        "# Scaling will adjust image brightness to avoid washed out portions in the resulting color transfer\n",
        "# that can be caused by clipping.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "clip = True\n",
        "\n",
        "# Should color transfer strictly follow methodology layed out in original paper?\n",
        "# The method does not always produce aesthetically pleasing results.\n",
        "# If False then L*a*b* components will be scaled using the reciprocal of the scaling factor proposed\n",
        "# in the paper. This method seems to produce more consistently aesthetically pleasing results.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "preserve_paper = True\n",
        "\n",
        "[color.manual_balance]\n",
        "# OPTIONS FOR MANUALLY ALTERING THE BALANCE OF COLORS OF THE SWAPPED FACE\n",
        "\n",
        "# The colorspace to use for adjustment: The three adjustment sliders will effect the image differently\n",
        "# depending on which colorspace is selected:\n",
        "#     - RGB: Red, Green, Blue. An additive colorspace where colors are obtained by a linear\n",
        "# \t\tcombination of Red, Green, and Blue values. The three channels are correlated by the amount of\n",
        "# \t\tlight hitting the surface. In RGB color space the color information is separated into three\n",
        "# \t\tchannels but the same three channels also encode brightness information.\n",
        "#     - HSV: Hue, Saturation, Value. Hue - Dominant wavelength. Saturation - Purity / shades of color.\n",
        "# \t\tValue - Intensity. Best thing is that it uses only one channel to describe color (H), making it\n",
        "# \t\tvery intuitive to specify color.\n",
        "#     - LAB: Lightness, A, B. Lightness - Intensity. A - Color range from green to magenta. B - Color\n",
        "# \t\trange from blue to yellow. The L channel is independent of color information and encodes\n",
        "# \t\tbrightness only. The other two channels encode color.\n",
        "#     - YCrCb: Y - Luminance or Luma component obtained from RGB after gamma correction. Cr - how far\n",
        "# \t\tis the red component from Luma. Cb - how far is the blue component from Luma. Separates the\n",
        "# \t\tluminance and chrominance components into different channels.\n",
        "# \n",
        "# Choose from: ['RGB', 'HSV', 'LAB', 'YCrCb']\n",
        "# [Default: HSV]\n",
        "colorspace = HSV\n",
        "\n",
        "# Balance of channel 1:\n",
        "#     - RGB: Red\n",
        "#     - HSV: Hue\n",
        "#     - LAB: Lightness\n",
        "#     - YCrCb: Luma\n",
        "# \n",
        "# Select a decimal number between -100.0 and 100.0\n",
        "# [Default: 0.0]\n",
        "balance_1 = 0.0\n",
        "\n",
        "# Balance of channel 2:\n",
        "#     - RGB: Green\n",
        "#     - HSV: Saturation\n",
        "#     - LAB: Green > Magenta\n",
        "#     - YCrCb: Distance of red from Luma\n",
        "# \n",
        "# Select a decimal number between -100.0 and 100.0\n",
        "# [Default: 0.0]\n",
        "balance_2 = 0.0\n",
        "\n",
        "# Balance of channel 3:\n",
        "#     - RGB: Blue\n",
        "#     - HSV: Intensity\n",
        "#     - LAB: Blue > Yellow\n",
        "#     - YCrCb: Distance of blue from Luma\n",
        "# \n",
        "# Select a decimal number between -100.0 and 100.0\n",
        "# [Default: 0.0]\n",
        "balance_3 = 0.0\n",
        "\n",
        "# Amount of contrast applied.\n",
        "# \n",
        "# Select a decimal number between -100.0 and 100.0\n",
        "# [Default: 0.0]\n",
        "contrast = 0.0\n",
        "\n",
        "# Amount of brighness applied.\n",
        "# \n",
        "# Select a decimal number between -100.0 and 100.0\n",
        "# [Default: 0.0]\n",
        "brightness = 0.0\n",
        "\n",
        "[color.match_hist]\n",
        "# OPTIONS FOR MATCHING THE HISTOGRAMS BETWEEN THE SOURCE AND DESTINATION FACES\n",
        "\n",
        "# Adjust the threshold for histogram matching. Can reduce extreme colors leaking in by filtering out\n",
        "# colors at the extreme ends of the histogram spectrum.\n",
        "# \n",
        "# Select a decimal number between 90.0 and 100.0\n",
        "# [Default: 99.0]\n",
        "threshold = 99.0\n",
        "\n",
        "[mask.box_blend]\n",
        "# OPTIONS FOR BLENDING THE EDGES OF THE SWAPPED BOX WITH THE BACKGROUND IMAGE\n",
        "\n",
        "# The type of blending to use:\n",
        "#     - gaussian: Blend with Gaussian filter. Slower, but often better than Normalized\n",
        "#     - normalized: Blend with Normalized box filter. Faster than Gaussian\n",
        "#     - none: Don't perform blending\n",
        "# \n",
        "# Choose from: ['gaussian', 'normalized', 'none']\n",
        "# [Default: gaussian]\n",
        "type = gaussian\n",
        "\n",
        "# The distance from the edges of the swap box to start blending.\n",
        "# The distance is set as percentage of the swap box size to give the number of pixels from the edge of\n",
        "# the box. Eg: For a swap area of 256px and a percentage of 4%, blending would commence 10 pixels from\n",
        "# the edge.\n",
        "# Higher percentages start the blending from closer to the center of the face, so will reveal more of\n",
        "# the source face.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 25.0\n",
        "# [Default: 11.0]\n",
        "distance = 11.0\n",
        "\n",
        "# Radius dictates how much blending should occur, or more specifically, how far the blending will\n",
        "# spread away from the 'distance' parameter.\n",
        "# This figure is set as a percentage of the swap box size to give the radius in pixels. Eg: For a swap\n",
        "# area of 256px and a percentage of 5%, the radius would be 13 pixels.\n",
        "# NB: Higher percentage means more blending, but too high may reveal more of the source face, or lead\n",
        "# to hard lines at the border.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 25.0\n",
        "# [Default: 5.0]\n",
        "radius = 5.0\n",
        "\n",
        "# The number of passes to perform. Additional passes of the blending algorithm can improve smoothing\n",
        "# at a time cost. This is more useful for 'box' type blending.\n",
        "# Additional passes have exponentially less effect so it's not worth setting this too high.\n",
        "# \n",
        "# Select an integer between 1 and 8\n",
        "# [Default: 1]\n",
        "passes = 1\n",
        "\n",
        "[mask.mask_blend]\n",
        "# OPTIONS FOR BLENDING THE EDGES BETWEEN THE MASK AND THE BACKGROUND IMAGE\n",
        "\n",
        "# The type of blending to use:\n",
        "#     - gaussian: Blend with Gaussian filter. Slower, but often better than Normalized\n",
        "#     - normalized: Blend with Normalized box filter. Faster than Gaussian\n",
        "#     - none: Don't perform blending\n",
        "# \n",
        "# Choose from: ['gaussian', 'normalized', 'none']\n",
        "# [Default: normalized]\n",
        "type = normalized\n",
        "\n",
        "# The kernel size dictates how much blending should occur.\n",
        "# The size is the diameter of the kernel in pixels (calculated from a 128px mask). This value should\n",
        "# be odd, if an even number is passed in then it will be rounded to the next odd number. Higher sizes\n",
        "# means more blending.\n",
        "# \n",
        "# Select an integer between 1 and 9\n",
        "# [Default: 3]\n",
        "kernel_size = 3\n",
        "\n",
        "# The number of passes to perform. Additional passes of the blending algorithm can improve smoothing\n",
        "# at a time cost. This is more useful for 'box' type blending.\n",
        "# Additional passes have exponentially less effect so it's not worth setting this too high.\n",
        "# \n",
        "# Select an integer between 1 and 8\n",
        "# [Default: 4]\n",
        "passes = 4\n",
        "\n",
        "# Sets pixels that are near white to white and near black to black. Set to 0 for off.\n",
        "# \n",
        "# Select an integer between 0 and 50\n",
        "# [Default: 4]\n",
        "threshold = 4\n",
        "\n",
        "# Erosion kernel size as a percentage of the mask radius area.\n",
        "# Positive values apply erosion which reduces the size of the swapped area.\n",
        "# Negative values apply dilation which increases the swapped area.\n",
        "# \n",
        "# Select a decimal number between -100.0 and 100.0\n",
        "# [Default: 0.0]\n",
        "erosion = 0.0\n",
        "\n",
        "[scaling.sharpen]\n",
        "# OPTIONS FOR SHARPENING THE FACE AFTER PLACEMENT\n",
        "\n",
        "# The type of sharpening to use:\n",
        "#     - none: Don't perform any sharpening.\n",
        "#     - box: Fastest, but weakest method. Uses a box filter to assess edges.\n",
        "#     - gaussian: Slower, but better than box. Uses a gaussian filter to assess edges.\n",
        "#     - unsharp-mask: Slowest, but most tweakable. Uses the unsharp-mask method to assess edges.\n",
        "# \n",
        "# Choose from: ['none', 'box', 'gaussian', 'unsharp_mask']\n",
        "# [Default: none]\n",
        "method = none\n",
        "\n",
        "# Percentage that controls the magnitude of each overshoot (how much darker and how much lighter the\n",
        "# edge borders become).\n",
        "# This can also be thought of as how much contrast is added at the edges. It does not affect the width\n",
        "# of the edge rims.\n",
        "# \n",
        "# Select an integer between 100 and 500\n",
        "# [Default: 150]\n",
        "amount = 150\n",
        "\n",
        "# Affects the size of the edges to be enhanced or how wide the edge rims become, so a smaller radius\n",
        "# enhances smaller-scale detail.\n",
        "# Radius is set as a percentage of the final frame width and rounded to the nearest pixel. E.g for a\n",
        "# 1280 width frame, a 0.6 percenatage will give a radius of 8px.\n",
        "# Higher radius values can cause halos at the edges, a detectable faint light rim around objects. Fine\n",
        "# detail needs a smaller radius.\n",
        "# Radius and amount interact; reducing one allows more of the other.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 5.0\n",
        "# [Default: 0.3]\n",
        "radius = 0.3\n",
        "\n",
        "# [unsharp_mask only] Controls the minimal brightness change that will be sharpened or how far apart\n",
        "# adjacent tonal values have to be before the filter does anything.\n",
        "# This lack of action is important to prevent smooth areas from becoming speckled. The threshold\n",
        "# setting can be used to sharpen more pronounced edges, while leaving subtler edges untouched.\n",
        "# Low values should sharpen more because fewer areas are excluded.\n",
        "# Higher threshold values exclude areas of lower contrast.\n",
        "# \n",
        "# Select a decimal number between 1.0 and 10.0\n",
        "# [Default: 5.0]\n",
        "threshold = 5.0\n",
        "\n",
        "[writer.ffmpeg]\n",
        "# OPTIONS FOR ENCODING CONVERTED FRAMES TO VIDEO.\n",
        "\n",
        "# Video container to use.\n",
        "# \n",
        "# Choose from: ['avi', 'flv', 'mkv', 'mov', 'mp4', 'mpeg', 'webm']\n",
        "# [Default: mp4]\n",
        "container = mp4\n",
        "\n",
        "# Video codec to use:\n",
        "#     - libx264: H.264. A widely supported and commonly used codec.\n",
        "#     - libx265: H.265 / HEVC video encoder application library.\n",
        "# \n",
        "# Choose from: ['libx264', 'libx265']\n",
        "# [Default: libx264]\n",
        "codec = libx264\n",
        "\n",
        "# Constant Rate Factor:  0 is lossless and 51 is worst quality possible. A lower value generally leads\n",
        "# to higher quality, and a subjectively sane range is 17-28. Consider 17 or 18 to be visually lossless\n",
        "# or nearly so; it should look the same or nearly the same as the input but it isn't technically\n",
        "# lossless.\n",
        "# The range is exponential, so increasing the CRF value +6 results in roughly half the bitrate / file\n",
        "# size, while -6 leads to roughly twice the bitrate.\n",
        "# \n",
        "# Select an integer between 0 and 51\n",
        "# [Default: 23]\n",
        "crf = 23\n",
        "\n",
        "# A preset is a collection of options that will provide a certain encoding speed to compression ratio.\n",
        "# A slower preset will provide better compression (compression is quality per filesize).\n",
        "# Use the slowest preset that you have patience for.\n",
        "# \n",
        "# Choose from: ['ultrafast', 'superfast', 'veryfast', 'faster', 'fast', 'medium', 'slow', 'slower',\n",
        "# 'veryslow']\n",
        "# [Default: medium]\n",
        "preset = medium\n",
        "\n",
        "# Change settings based upon the specifics of your input:\n",
        "#     - none: Don't perform any additional tuning.\n",
        "#     - film: [H.264 only] Use for high quality movie content; lowers deblocking.\n",
        "#     - animation: [H.264 only] Good for cartoons; uses higher deblocking and more reference frames.\n",
        "#     - grain: Preserves the grain structure in old, grainy film material.\n",
        "#     - stillimage: [H.264 only] Good for slideshow-like content.\n",
        "#     - fastdecode: Allows faster decoding by disabling certain filters.\n",
        "#     - zerolatency: Good for fast encoding and low-latency streaming.\n",
        "# \n",
        "# Choose from: ['none', 'film', 'animation', 'grain', 'stillimage', 'fastdecode', 'zerolatency']\n",
        "# [Default: none]\n",
        "tune = none\n",
        "\n",
        "# [H.264 Only] Limit the output to a specific H.264 profile. Don't change this unless your target\n",
        "# device only supports a certain profile.\n",
        "# \n",
        "# Choose from: ['auto', 'baseline', 'main', 'high', 'high10', 'high422', 'high444']\n",
        "# [Default: auto]\n",
        "profile = auto\n",
        "\n",
        "# [H.264 Only] Set the encoder level, Don't change this unless your target device only supports a\n",
        "# certain level.\n",
        "# \n",
        "# Choose from: ['auto', '1', '1b', '1.1', '1.2', '1.3', '2', '2.1', '2.2', '3', '3.1', '3.2', '4',\n",
        "# '4.1', '4.2', '5', '5.1', '5.2', '6', '6.1', '6.2']\n",
        "# [Default: auto]\n",
        "level = auto\n",
        "\n",
        "# Skip muxing audio to the final video output. This will result in a video without an audio track.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "skip_mux = False\n",
        "\n",
        "[writer.gif]\n",
        "# OPTIONS FOR OUTPUTTING CONVERTED FRAMES TO AN ANIMATED GIF.\n",
        "\n",
        "# Frames per Second.\n",
        "# \n",
        "# Select an integer between 1 and 60\n",
        "# [Default: 25]\n",
        "fps = 25\n",
        "\n",
        "# The number of iterations. Set to 0 to loop indefinitely.\n",
        "# \n",
        "# Select an integer between 0 and 100\n",
        "# [Default: 0]\n",
        "loop = 0\n",
        "\n",
        "# The number of colors to quantize the image to. Is rounded to the nearest power of two.\n",
        "# \n",
        "# Choose from: ['2', '4', '8', '16', '32', '64', '128', '256']\n",
        "# [Default: 256]\n",
        "palettesize = 256\n",
        "\n",
        "# If True, will try and optimize the GIF by storing only the rectangular parts of each frame that\n",
        "# change with respect to the previous.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "subrectangles = False\n",
        "\n",
        "[writer.opencv]\n",
        "# OPTIONS FOR OUTPUTTING CONVERTED FRAMES TO A SERIES OF IMAGES USING OPENCV\n",
        "# OPENCV CAN BE FASTER THAN OTHER IMAGE WRITERS, BUT LACKS SOME CONFIGURATION OPTIONS AND FORMATS.\n",
        "\n",
        "# Image format to use:\n",
        "#     - bmp: Windows bitmap\n",
        "#     - jpg: JPEG format\n",
        "#     - jp2: JPEG 2000 format\n",
        "#     - png: Portable Network Graphics\n",
        "#     - ppm: Portable Pixmap Format\n",
        "# \n",
        "# Choose from: ['bmp', 'jpg', 'jp2', 'png', 'ppm']\n",
        "# [Default: png]\n",
        "format = png\n",
        "\n",
        "# Place the swapped face on a transparent layer rather than the original frame.\n",
        "# NB: This is only compatible with images saved in png format. If an incompatible format is selected\n",
        "# then the image will be saved as a png.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "draw_transparent = False\n",
        "\n",
        "# [jpg only] Set the jpg quality. 1 is worst 95 is best. Higher quality leads to larger file sizes.\n",
        "# \n",
        "# Select an integer between 1 and 95\n",
        "# [Default: 75]\n",
        "jpg_quality = 75\n",
        "\n",
        "# [png only] ZLIB compression level, 1 gives best speed, 9 gives best compression, 0 gives no\n",
        "# compression at all.\n",
        "# \n",
        "# Select an integer between 0 and 9\n",
        "# [Default: 3]\n",
        "png_compress_level = 3\n",
        "\n",
        "[writer.pillow]\n",
        "# OPTIONS FOR OUTPUTTING CONVERTED FRAMES TO A SERIES OF IMAGES USING PILLOW\n",
        "# PILLOW IS MORE FEATURE RICH THAN OPENCV BUT CAN BE SLOWER.\n",
        "\n",
        "# Image format to use:\n",
        "#     - bmp: Windows bitmap\n",
        "#     - gif: Graphics Interchange Format (NB: Not animated)\n",
        "#     - jpg: JPEG format\n",
        "#     - jp2: JPEG 2000 format\n",
        "#     - png: Portable Network Graphics\n",
        "#     - ppm: Portable Pixmap Format\n",
        "#     - tif: Tag Image File Format\n",
        "# \n",
        "# Choose from: ['bmp', 'gif', 'jpg', 'jp2', 'png', 'ppm', 'tif']\n",
        "# [Default: png]\n",
        "format = png\n",
        "\n",
        "# Place the swapped face on a transparent layer rather than the original frame.\n",
        "# NB: This is only compatible with images saved in png or tif format. If an incompatible format is\n",
        "# selected then the image will be saved as a png.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "draw_transparent = False\n",
        "\n",
        "# [gif, jpg and png only] If enabled, indicates that the encoder should make an extra pass over the\n",
        "# image in order to select optimal encoder settings.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "optimize = False\n",
        "\n",
        "# [gif only] Set whether to save the gif as interlaced or not.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "gif_interlace = True\n",
        "\n",
        "# [jpg only] Set the jpg quality. 1 is worst 95 is best. Higher quality leads to larger file sizes.\n",
        "# \n",
        "# Select an integer between 1 and 95\n",
        "# [Default: 75]\n",
        "jpg_quality = 75\n",
        "\n",
        "# [png only] ZLIB compression level, 1 gives best speed, 9 gives best compression, 0 gives no\n",
        "# compression at all. When optimize option is set to True this has no effect (it is set to 9\n",
        "# regardless of a value passed).\n",
        "# \n",
        "# Select an integer between 0 and 9\n",
        "# [Default: 3]\n",
        "png_compress_level = 3\n",
        "\n",
        "# [tif only] The desired compression method for the file.\n",
        "# \n",
        "# Choose from: ['none', 'tiff_ccitt', 'group3', 'group4', 'tiff_jpeg', 'tiff_adobe_deflate',\n",
        "# 'tiff_thunderscan', 'tiff_deflate', 'tiff_sgilog', 'tiff_sgilog24', 'tiff_raw_16']\n",
        "# [Default: tiff_deflate]\n",
        "tif_compression = tiff_deflate\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"faceswap/config/convert.ini\", \"w\") as text_file:\n",
        "    text_file.write(config)"
      ],
      "metadata": {
        "id": "kR359HFiNtfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sWnIRM0tNd_4",
        "A0jMFCeJMr1h",
        "T2CcguuPNmdt"
      ],
      "name": "FaceColab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}