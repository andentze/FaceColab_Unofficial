{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceColab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkI0eNx37Yjh0gsc+t37uI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andentze/FaceColab_Unofficial/blob/main/FaceColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7-0WoBC71P8"
      },
      "source": [
        "# Faceswap Notebook\n",
        " \n",
        "Welcome to the Faceswap *not official* notebook. Ever had a problem with your PC/laptop not being good enough for Faceswap? This notebook will help you out with **everything** regarding Faceswap.\n",
        " \n",
        " \n",
        "Feel free to set up the environment as you like. I, personally, prefer my dataset and the model being on the Google Drive directly, or something like this:\n",
        " \n",
        "> \"/content/drive/My Drive/colab/faceswap/model\n",
        " \n",
        "> \"/content/drive/My Drive/colab/faceswap/faces/A\n",
        " \n",
        "> /content/drive/My Drive/colab/faceswap/faces/B\n",
        " \n",
        "> /content/drive/My Drive/colab/faceswap/config\n",
        " \n",
        "and so on.\n",
        " \n",
        "You can choose to either use the GUI version of Faceswap(*which can take some time to set up*) or the CLI version of Faceswap that is present here.\n",
        " \n",
        "You will also receive 4 kinds of GPUs: P4, T4, K80 or P100. Also, you get 12 hours of notebook usage until you get disconnected(24 hours in Colab Pro). You *might* get frequent disconnects.\n",
        " \n",
        "And before you start using this notebook, clone it to your Google Drive. (File -> Save a copy in Drive)\n",
        "\n",
        "**I TAKE NO CREDIT FOR ANY REPOSITORIES, NOR SOFTWARE USED IN THIS NOTEBOOK. ANYTHING USED HERE IS NOT MINE.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TctjDRl8d3N"
      },
      "source": [
        "#@title Check the current GPU\n",
        "#@markdown Right here you can check what kind of GPU you have available right now. Run this code to output the GPU used.\n",
        "#@markdown If it errors out, make sure your runtime type is set to \"GPU\".\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btFh5tBGExWQ",
        "cellView": "form"
      },
      "source": [
        "#@title Keep-Alive Script\n",
        "#@markdown You have to activate this to automatically reconnect if Colab disconnects you. Run this code, to activate the \"Keep-Alive\" script.\n",
        "#@markdown And just to make sure, open the console using Ctrl+Shift+I, and paste the code below. This will work efficently for both CLI and DE.\n",
        " \n",
        "import IPython\n",
        "from google.colab import output\n",
        " \n",
        "display(IPython.display.Javascript('''\n",
        "function ClickConnect() {\n",
        "  console.log('Working')\n",
        "  document\n",
        "    .querySelector('#top-toolbar > colab-connect-button')\n",
        "    .shadowRoot.querySelector('#connect')\n",
        "    .click()\n",
        "}\n",
        " \n",
        "setInterval(ClickConnect, 60000)\n",
        "'''))\n",
        " \n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnAN0NQAGe49"
      },
      "source": [
        "> function ClickConnect() {\n",
        "\n",
        ">  console.log('Working')\n",
        "\n",
        ">  document\n",
        "\n",
        ">    .querySelector('#top-toolbar > colab-connect-button')\n",
        "\n",
        ">    .shadowRoot.querySelector('#connect')\n",
        "\n",
        ">    .click()\n",
        "}\n",
        "\n",
        ">    setInterval(ClickConnect, 60000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6YcIgZV_3xA"
      },
      "source": [
        "# Setting up the Desktop Environment\n",
        " \n",
        "Now that we know we have a GPU available, we're set. Now, **you** choose what kind of environment you need or want.\n",
        " \n",
        "*   Desktop Environment\n",
        "*   Command Line(CLI)\n",
        " \n",
        "A little side-note:\n",
        " \n",
        "*   Desktop Environments take 5-8 minutes to set up, while CLI takes around 4 minutes.\n",
        "*   Desktop Environments are a bit unstable, as Colab was not intended to use in such way. So, be careful.\n",
        " \n",
        "The RDP snippet was taken from this repository:\n",
        "https://github.com/PradyumnaKrishna/Colab-Hacks/\n",
        "\n",
        "**YET TO BE IMPLEMENTED COMPLETELY, DO NOT USE UNTIL IT IS DONE. CLI IS USABLE FOR NOW**\n",
        "\n",
        "**NOT RECOMMENDED TO USE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDQlUuKeBbX6",
        "cellView": "form"
      },
      "source": [
        "#@title Create a User\n",
        "#@markdown Create a user here, you can leave the name and the password as default.\n",
        "#@markdown This is **required** for the DE(Desktop Environment).\n",
        " \n",
        "import os\n",
        " \n",
        "username = \"user\" #@param {type:\"string\"}\n",
        "password = \"root\" #@param {type:\"string\"}\n",
        " \n",
        "print(\"Creating User and Setting it up\")\n",
        " \n",
        "# Creation of user\n",
        "os.system(f\"useradd -m {username}\")\n",
        " \n",
        "# Add user to sudo group\n",
        "os.system(f\"adduser {username} sudo\")\n",
        "    \n",
        "# Set password of user to 'root'\n",
        "os.system(f\"echo '{username}:{password}' | sudo chpasswd\")\n",
        " \n",
        "# Change default shell from sh to bash\n",
        "os.system(\"sed -i 's/\\/bin\\/sh/\\/bin\\/bash/g' /etc/passwd\")\n",
        " \n",
        "print(\"User Created and Configured\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ71d37CCOPr",
        "cellView": "form"
      },
      "source": [
        "#@title Install the RDP\n",
        "#@markdown Now, you can install RDP. Visit http://remotedesktop.google.com/headless and copy the command for Debian after authentication. Then, run this code. It will take 5-8 minutes to install.\n",
        " \n",
        "import os\n",
        "import subprocess\n",
        " \n",
        "CRP = \"DISPLAY= /opt/google/chrome-remote-desktop/start-host --code=\\\"4/0AY0e-g7_v-8XXFg429fOGy2bLQ_93HV7FdbGr7k6RtwqUEkzAoH6J0FH1MUmFZ3wyu7fNQ\\\" --redirect-url=\\\"https://remotedesktop.google.com/_/oauthredirect\\\" --name=$(hostname)\" #@param {type:\"string\"}\n",
        " \n",
        "#@markdown Enter a pin more or equal to 6 digits.\n",
        "Pin = 123456 #@param {type: \"integer\"}\n",
        " \n",
        " \n",
        "class CRD:\n",
        "    def __init__(self):\n",
        "        os.system(\"apt update\")\n",
        "        self.installCRD()\n",
        "        self.installDesktopEnvironment()\n",
        "        self.installGoogleChorme()\n",
        "        self.finish()\n",
        " \n",
        "    @staticmethod\n",
        "    def installCRD():\n",
        "        print(\"Installing Chrome Remote Desktop\")\n",
        "        subprocess.run(['wget', 'https://dl.google.com/linux/direct/chrome-remote-desktop_current_amd64.deb'], stdout=subprocess.PIPE)\n",
        "        subprocess.run(['dpkg', '--install', 'chrome-remote-desktop_current_amd64.deb'], stdout=subprocess.PIPE)\n",
        "        subprocess.run(['apt', 'install', '--assume-yes', '--fix-broken'], stdout=subprocess.PIPE)\n",
        " \n",
        "    @staticmethod\n",
        "    def installDesktopEnvironment():\n",
        "        print(\"Installing Desktop Environment\")\n",
        "        os.system(\"export DEBIAN_FRONTEND=noninteractive\")\n",
        "        os.system(\"apt install --assume-yes xfce4 desktop-base xfce4-terminal\")\n",
        "        os.system(\"bash -c 'echo \\\"exec /etc/X11/Xsession /usr/bin/xfce4-session\\\" > /etc/chrome-remote-desktop-session'\")\n",
        "        os.system(\"apt remove --assume-yes gnome-terminal\")\n",
        "        os.system(\"apt install --assume-yes xscreensaver\")\n",
        "        os.system(\"systemctl disable lightdm.service\")\n",
        " \n",
        "    @staticmethod\n",
        "    def installGoogleChorme():\n",
        "        print(\"Installing Google Chrome\")\n",
        "        subprocess.run([\"wget\", \"https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\"], stdout=subprocess.PIPE)\n",
        "        subprocess.run([\"dpkg\", \"--install\", \"google-chrome-stable_current_amd64.deb\"], stdout=subprocess.PIPE)\n",
        "        subprocess.run(['apt', 'install', '--assume-yes', '--fix-broken'], stdout=subprocess.PIPE)\n",
        " \n",
        "    @staticmethod\n",
        "    def finish():\n",
        "        print(\"Finalizing\")\n",
        "        os.system(f\"adduser {username} chrome-remote-desktop\")\n",
        "        command = f\"{CRP} --pin={Pin}\"\n",
        "        os.system(f\"su - {username} -c '{command}'\")\n",
        "        os.system(\"service chrome-remote-desktop start\")\n",
        "        print(\"Finished Succesfully\")\n",
        " \n",
        " \n",
        "try:\n",
        "    if username:\n",
        "        if CRP == \"\":\n",
        "            print(\"Please enter authcode from the given link\")\n",
        "        elif len(str(Pin)) < 6:\n",
        "            print(\"Enter a pin more or equal to 6 digits\")\n",
        "        else:\n",
        "            CRD()\n",
        "except NameError as e:\n",
        "    print(\"username variable not found\")\n",
        "    print(\"Create a User First\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RidH0tfrDBg6",
        "cellView": "form"
      },
      "source": [
        "#@title Mount the Google Drive\n",
        "#@markdown Google Drive is used as Persistance HDD for files. It is mounted at your `user` folder. (*`root` if the username was not set*)\n",
        "#@markdown This is for DE, look farther down for CLI.\n",
        " \n",
        "def MountGDrive():\n",
        "    from google.colab import drive\n",
        " \n",
        "    ! runuser -l $user -c \"yes | python3 -m pip install --user google-colab\"  > /dev/null 2>&1\n",
        " \n",
        "    mount = \"\"\"from os import environ as env\n",
        "from google.colab import drive\n",
        " \n",
        "env['CLOUDSDK_CONFIG']  = '/content/.config'\n",
        "drive.mount('{}')\"\"\".format(mountpoint)\n",
        " \n",
        "    with open('/content/mount.py', 'w') as script:\n",
        "        script.write(mount)\n",
        " \n",
        "    ! runuser -l $user -c \"python3 /content/mount.py\"\n",
        " \n",
        "try:\n",
        "    if username:\n",
        "        mountpoint = \"/home/\"+username+\"/drive\"\n",
        "        user = username\n",
        "except NameError:\n",
        "    print(\"username variable not found, mounting at `/content/drive' using `root'\")\n",
        "    mountpoint = '/content/drive'\n",
        "    user = 'root'\n",
        " \n",
        "MountGDrive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5jP2nJCEiX5"
      },
      "source": [
        "From there, you can install Faceswap *easily*. Or, you can use this to clean your dataset. But. Don't forget that Colab terminates your session if it's not active. So make sure your \"Keep-Alive\" script is active.\n",
        "\n",
        "*After checking the installation, turns out it doesn't like a couple of dependicies.*\n",
        "\n",
        "(yet to be checked and fixed if necessary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcGpeieHXHb"
      },
      "source": [
        "# Setting up the CLI\n",
        "\n",
        "Not a fan of the GUI or it simply doesn't work? Look no furher. From this point you will install Faceswap directly to Colab without the need of the DE.\n",
        "\n",
        "It was set up so even a toddler could figure out how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ZS8LK0IjPK",
        "cellView": "form"
      },
      "source": [
        "#@title Install Faceswap\n",
        "#@markdown Install Faceswap using this code snippet. Even more simplier than mounting, just with a simple click.\n",
        "#@markdown This will also copy your configuration files from your Drive, so specify the directory to each .ini file.\n",
        "\n",
        "#@markdown It will also ask for a Google authentication with a prompt. That is for a Google Drive mounting. You need this for persistance storage. Make sure your Drive is ready to use.\n",
        "train = \"train.ini dir(/content/dir/train.ini)\" #@param {type:\"string\"}\n",
        "extract = \"extract.ini dir(/content/dir/extract.ini)\" #@param {type:\"string\"}\n",
        "convert = \"extract.ini dir(/content/dir/convert.ini)\" #@param {type:\"string\"}\n",
        "#@markdown If you install Faceswap to your drive, you'll have to tick \"drive_install\" on every code chunk. But, it saves your time so you won't have to install Faceswap again everytime you crash.\n",
        "install_to_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/deepfakes/faceswap.git\n",
        "!git clone https://github.com/andentze/facecolab_requirements\n",
        "!cp \"{train}\" faceswap/config/\n",
        "!cp \"{extract}\" faceswap/config/\n",
        "!cp \"{convert}\" faceswap/config/\n",
        "clear_output()\n",
        "#!pip install -r faceswap/requirements_nvidia.txt\n",
        "#!pip uninstall -y imageio\n",
        "#!pip uninstall -y imageio-ffmpeg\n",
        "!pip uninstall -y tensorflow\n",
        "#!pip uninstall -y tensorflow-gpu\n",
        "\n",
        "!pip install -r \"/content/facecolab_requirements/requirements_nvidia.txt\"\n",
        "#!pip install \"imageio>=2.9.0\"\n",
        "#!pip install \"imageio-ffmpeg<0.4.3\"\n",
        "#!pip install \"ffmpy==0.2.3\"\n",
        "#!pip install \"tensorflow-gpu>=2.2.0,<2.5.0\"\n",
        "\n",
        "if install_to_drive:\n",
        "  !cp \"/content/faceswap\" \"/content/drive/My Drive/\" -r\n",
        "  print(\"Installed to Google Drive.\")\n",
        "  print(\"Deleting the old faceswap folder.\")\n",
        "  !rm -r \"/content/faceswap/\"\n",
        "%env FACESWAP_BACKEND = \"nvidia\"\n",
        "clear_output()\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvi8sfssLKR8"
      },
      "source": [
        "And with that, you're good to go! You can go ahead, and use Faceswap at your own will!(*that is, until you hit a 12 hour mark.*)\n",
        "\n",
        "**It appears Colab doesn't like long-terms calculations. So it will disconnect you at the most random times. BE AWARE OF THAT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgIDaoDPemKr"
      },
      "source": [
        "# Workflow\n",
        " \n",
        "Welcome to the Faceswap environment. Your first step is to extract the faces from your footage. After which you're going to have to fix your dataset. While PC/laptop users can fix that locally, mobile users will have to use the DE to do so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zta3jdNf7Wlf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "323ba9ff-5cd0-4547-b49d-602c5d47355d"
      },
      "source": [
        "#@title Extraction\n",
        "#@markdown Extraction is the first step for dataset creation. You **must** have good data in order for training to work better. Use this code snippet to extract your dataset.\n",
        " \n",
        "#set variables start\n",
        " \n",
        "input_dir = \"/content/drive/MyDrive/colab_files/faceswap/videos/A/data_dst.mp4\" #@param {type:\"string\"}\n",
        "output_dir = \"/content/drive/MyDrive/colab_files/faceswap/faces/A\" #@param {type:\"string\"}\n",
        "detector = \"s3fd\" #@param [\"cv2-dnn\", \"mtcnn\", \"s3fd\"]\n",
        "alignment = \"fan\" #@param [\"cv2dnn\", \"fan\"]\n",
        "masker = \"unet-dfl\" #@param [\"bisenet-fp\", \"unet-dfl\", \"vgg-clear\", \"vgg-obstructed\"]\n",
        "al_normalization = \"clahe\" #@param [\"clahe\", \"hist\", \"mean\"]\n",
        "refeed = 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "size = 512 #@param {type:\"slider\", min:256, max:2048, step:64}\n",
        "extract_every_n = 1 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "drive_install = False #@param {type:\"boolean\"}\n",
        "if drive_install:\n",
        "  path = \"/content/drive/My Drive/faceswap/\"\n",
        "  print(\"The installation is on Google Drive. Using Drive directory.\")\n",
        "if not drive_install:\n",
        "  path = \"/content/faceswap/\"\n",
        "  print(\"The installation is local. Using local directory.\")\n",
        "#set variables end\n",
        "\n",
        "!python3 '{path}'faceswap.py extract \\\n",
        "-i '{input_dir}' \\\n",
        "-o '{output_dir}' \\\n",
        "-D '{detector}' \\\n",
        "-A '{alignment}' \\\n",
        "-M '{masker}' \\\n",
        "-nm '{al_normalization}' \\\n",
        "-rf '{refeed}' \\\n",
        "-min 20 -l 0.4 \\\n",
        "-sz '{size}' \\\n",
        "-een '{extract_every_n}' \\\n",
        "-si 0 -ssf -L INFO"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The installation is local. Using local directory.\n",
            "Setting Faceswap backend from environment variable to \"NVIDIA\"\n",
            "usage: faceswap.py extract [-h] [-X {0} [{0} ...]] [-C CONFIGFILE]\n",
            "                           [-L {INFO,VERBOSE,DEBUG,TRACE}] [-LF LOGFILE] -i\n",
            "                           INPUT_DIR -o OUTPUT_DIR [-al ALIGNMENTS_PATH]\n",
            "                           [-D {cv2-dnn,mtcnn,s3fd}] [-A {cv2-dnn,fan}]\n",
            "                           [-M {bisenet-fp,unet-dfl,vgg-clear,vgg-obstructed} [{bisenet-fp,unet-dfl,vgg-clear,vgg-obstructed} ...]]\n",
            "                           [-nm {none,clahe,hist,mean}] [-rf RE_FEED]\n",
            "                           [-r ROTATE_IMAGES] [-min MIN_SIZE]\n",
            "                           [-n NFILTER [NFILTER ...]] [-f FILTER [FILTER ...]]\n",
            "                           [-l REF_THRESHOLD] [-sz SIZE]\n",
            "                           [-een EXTRACT_EVERY_N] [-si SAVE_INTERVAL] [-dl]\n",
            "                           [-s] [-sf] [-ssf]\n",
            "\n",
            "Extract the faces from pictures or a video\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -X {0} [{0} ...], --exclude-gpus {0} [{0} ...]\n",
            "                        Exclude GPUs from use by Faceswap. Select the\n",
            "                        number(s) which correspond to any GPU(s) that you do\n",
            "                        not wish to be made available to Faceswap. Selecting\n",
            "                        all GPUs here will force Faceswap into CPU mode.\n",
            "                          - 0: Tesla K80\n",
            "  -C CONFIGFILE, --configfile CONFIGFILE\n",
            "                        Optionally overide the saved config with the path to a\n",
            "                        custom config file.\n",
            "  -L {INFO,VERBOSE,DEBUG,TRACE}, --loglevel {INFO,VERBOSE,DEBUG,TRACE}\n",
            "                        Log level. Stick with INFO or VERBOSE unless you need\n",
            "                        to file an error report. Be careful with TRACE as it\n",
            "                        will generate a lot of data\n",
            "  -LF LOGFILE, --logfile LOGFILE\n",
            "                        Path to store the logfile. Leave blank to store in the\n",
            "                        faceswap folder\n",
            "  -i INPUT_DIR, --input-dir INPUT_DIR\n",
            "                        Input directory or video. Either a directory\n",
            "                        containing the image files you wish to process or path\n",
            "                        to a video file. NB: This should be the source\n",
            "                        video/frames NOT the source faces.\n",
            "  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n",
            "                        Output directory. This is where the converted files\n",
            "                        will be saved.\n",
            "  -al ALIGNMENTS_PATH, --alignments ALIGNMENTS_PATH\n",
            "                        Optional path to an alignments file. Leave blank if\n",
            "                        the alignments file is at the default location.\n",
            "  -D {cv2-dnn,mtcnn,s3fd}, --detector {cv2-dnn,mtcnn,s3fd}\n",
            "                        Detector to use. Some of these have configurable\n",
            "                        settings in '/config/extract.ini' or 'Settings >\n",
            "                        Configure Extract 'Plugins':\n",
            "                          - cv2-dnn: A CPU only extractor which is the least\n",
            "                            reliable and least resource intensive. Use this if\n",
            "                            not using a GPU and time is important.\n",
            "                          - mtcnn: Good detector. Fast on CPU, faster on GPU.\n",
            "                            Uses fewer resources than other GPU detectors but\n",
            "                            can often return more false positives.\n",
            "                          - s3fd: Best detector. Slow on CPU, faster on GPU.\n",
            "                            Can detect more faces and fewer false positives\n",
            "                            than other GPU detectors, but is a lot more\n",
            "                            resource intensive.\n",
            "  -A {cv2-dnn,fan}, --aligner {cv2-dnn,fan}\n",
            "                        Aligner to use.\n",
            "                          - cv2-dnn: A CPU only landmark detector. Faster,\n",
            "                            less resource intensive, but less accurate. Only\n",
            "                            use this if not using a GPU and time is important.\n",
            "                          - fan: Best aligner. Fast on GPU, slow on CPU.\n",
            "  -M {bisenet-fp,unet-dfl,vgg-clear,vgg-obstructed} [{bisenet-fp,unet-dfl,vgg-clear,vgg-obstructed} ...], --masker {bisenet-fp,unet-dfl,vgg-clear,vgg-obstructed} [{bisenet-fp,unet-dfl,vgg-clear,vgg-obstructed} ...]\n",
            "                        Additional Masker(s) to use. The masks generated here\n",
            "                        will all take up GPU RAM. You can select none, one or\n",
            "                        multiple masks, but the extraction may take longer the\n",
            "                        more you select. NB: The Extended and Components\n",
            "                        (landmark based) masks are automatically generated on\n",
            "                        extraction.\n",
            "                          - bisenet-fp: Relatively lightweight NN based mask\n",
            "                            that provides more refined control over the area\n",
            "                            to be masked including full head masking\n",
            "                            (configurable in mask settings).\n",
            "                          - vgg-clear: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces clear of\n",
            "                            obstructions. Profile faces and obstructions may\n",
            "                            result in sub-par performance.\n",
            "                          - vgg-obstructed: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces. The mask\n",
            "                            model has been specifically trained to recognize\n",
            "                            some facial obstructions (hands and eyeglasses).\n",
            "                            Profile faces may result in sub-par performance.\n",
            "                          - unet-dfl: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces. The mask\n",
            "                            model has been trained by community members and\n",
            "                            will need testing for further description. Profile\n",
            "                            faces may result in sub-par performance.\n",
            "                        The auto generated masks are as follows:\n",
            "                          - components: Mask designed to provide facial\n",
            "                            segmentation based on the positioning of landmark\n",
            "                            locations. A convex hull is constructed around the\n",
            "                            exterior of the landmarks to create a mask.\n",
            "                          - extended: Mask designed to provide facial\n",
            "                            segmentation based on the positioning of landmark\n",
            "                            locations. A convex hull is constructed around the\n",
            "                            exterior of the landmarks and the mask is extended\n",
            "                            upwards onto the forehead.\n",
            "                        (eg: `-M unet-dfl vgg-clear`, `--masker vgg-\n",
            "                        obstructed`)\n",
            "  -nm {none,clahe,hist,mean}, --normalization {none,clahe,hist,mean}\n",
            "                        Performing normalization can help the aligner better\n",
            "                        align faces with difficult lighting conditions at an\n",
            "                        extraction speed cost. Different methods will yield\n",
            "                        different results on different sets. NB: This does not\n",
            "                        impact the output face, just the input to the aligner.\n",
            "                          - none: Don't perform normalization on the face.\n",
            "                          - clahe: Perform Contrast Limited Adaptive Histogram\n",
            "                            Equalization on the face.\n",
            "                          - hist: Equalize the histograms on the RGB channels.\n",
            "                          - mean: Normalize the face colors to the mean.\n",
            "  -rf RE_FEED, --re-feed RE_FEED\n",
            "                        The number of times to re-feed the detected face into\n",
            "                        the aligner. Each time the face is re-fed into the\n",
            "                        aligner the bounding box is adjusted by a small\n",
            "                        amount. The final landmarks are then averaged from\n",
            "                        each iteration. Helps to remove 'micro-jitter' but at\n",
            "                        the cost of slower extraction speed. The more times\n",
            "                        the face is re-fed into the aligner, the less micro-\n",
            "                        jitter should occur but the longer extraction will\n",
            "                        take.\n",
            "  -r ROTATE_IMAGES, --rotate-images ROTATE_IMAGES\n",
            "                        If a face isn't found, rotate the images to try to\n",
            "                        find a face. Can find more faces at the cost of\n",
            "                        extraction speed. Pass in a single number to use\n",
            "                        increments of that size up to 360, or pass in a list\n",
            "                        of numbers to enumerate exactly what angles to check.\n",
            "  -min MIN_SIZE, --min-size MIN_SIZE\n",
            "                        Filters out faces detected below this size. Length, in\n",
            "                        pixels across the diagonal of the bounding box. Set to\n",
            "                        0 for off\n",
            "  -n NFILTER [NFILTER ...], --nfilter NFILTER [NFILTER ...]\n",
            "                        Optionally filter out people who you do not wish to\n",
            "                        process by passing in an image of that person. Should\n",
            "                        be a front portrait with a single person in the image.\n",
            "                        Multiple images can be added space separated. NB:\n",
            "                        Using face filter will significantly decrease\n",
            "                        extraction speed and its accuracy cannot be\n",
            "                        guaranteed.\n",
            "  -f FILTER [FILTER ...], --filter FILTER [FILTER ...]\n",
            "                        Optionally select people you wish to process by\n",
            "                        passing in an image of that person. Should be a front\n",
            "                        portrait with a single person in the image. Multiple\n",
            "                        images can be added space separated. NB: Using face\n",
            "                        filter will significantly decrease extraction speed\n",
            "                        and its accuracy cannot be guaranteed.\n",
            "  -l REF_THRESHOLD, --ref_threshold REF_THRESHOLD\n",
            "                        For use with the optional nfilter/filter files.\n",
            "                        Threshold for positive face recognition. Lower values\n",
            "                        are stricter. NB: Using face filter will significantly\n",
            "                        decrease extraction speed and its accuracy cannot be\n",
            "                        guaranteed.\n",
            "  -sz SIZE, --size SIZE\n",
            "                        The output size of extracted faces. Make sure that the\n",
            "                        model you intend to train supports your required size.\n",
            "                        This will only need to be changed for hi-res models.\n",
            "  -een EXTRACT_EVERY_N, --extract-every-n EXTRACT_EVERY_N\n",
            "                        Extract every 'nth' frame. This option will skip\n",
            "                        frames when extracting faces. For example a value of 1\n",
            "                        will extract faces from every frame, a value of 10\n",
            "                        will extract faces from every 10th frame.\n",
            "  -si SAVE_INTERVAL, --save-interval SAVE_INTERVAL\n",
            "                        Automatically save the alignments file after a set\n",
            "                        amount of frames. By default the alignments file is\n",
            "                        only saved at the end of the extraction process. NB:\n",
            "                        If extracting in 2 passes then the alignments file\n",
            "                        will only start to be saved out during the second\n",
            "                        pass. WARNING: Don't interrupt the script when writing\n",
            "                        the file because it might get corrupted. Set to 0 to\n",
            "                        turn off\n",
            "  -dl, --debug-landmarks\n",
            "                        Draw landmarks on the ouput faces for debugging\n",
            "                        purposes.\n",
            "  -s, --skip-existing   Skips frames that have already been extracted and\n",
            "                        exist in the alignments file\n",
            "  -sf, --skip-existing-faces\n",
            "                        Skip frames that already have detected faces in the\n",
            "                        alignments file\n",
            "  -ssf, --skip-saving-faces\n",
            "                        Skip saving the detected faces to disk. Just create an\n",
            "                        alignments file\n",
            "\n",
            "Questions and feedback: https://faceswap.dev/forum\n",
            "faceswap.py extract: error: argument -nm/--normalization: invalid choice: '' (choose from 'none', 'clahe', 'hist', 'mean')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6D42AWnZUc6",
        "cellView": "form"
      },
      "source": [
        "#@title Training\n",
        "#@markdown Now, you have your dataset at tip-top shape! All you have to do is start training now! But wait. Before you actually start, make sure your configuration was set right.\n",
        " \n",
        "#@markdown Made sure? Then get to training by setting up the variables first.\n",
        " \n",
        "#set variables start\n",
        "input_a = \"your/dir/here/A\" #@param {type:\"string\"}\n",
        "input_b = \"your/dir/here/B\" #@param {type:\"string\"}\n",
        "num_iterations = 1200000 #@param {type:\"slider\", min:100000, max:3000000, step:25000}\n",
        "save_every = 360 #@param {type:\"slider\", min:360, max:1800, step:360}\n",
        "save_model_every = 25000 #@param {type:\"slider\", min:25000, max:100000, step:25000}\n",
        "batch_num = 48 #@param {type:\"slider\", min:4, max:72, step:4}\n",
        " \n",
        "trainer_type = \"dfaker\" #@param [\"lightweight\", \"original\", \"iae\", \"dfaker\", \"dlight\", \"unbalanced\", \"dfl-h128\", \"villain\"]\n",
        " \n",
        "model_dir = \"/content/dir/your_model/\" #@param {type:\"string\"}\n",
        "timelapse_dir = \"/content/dir/timelapse/\" #@param {type:\"string\"}\n",
        "\n",
        "drive_install = False #@param {type:\"boolean\"}\n",
        "if drive_install:\n",
        "  path = \"/content/drive/My Drive/faceswap/\"\n",
        "  print(\"The installation is on Google Drive. Using Drive directory.\")\n",
        "if not drive_install:\n",
        "  path = \"/content/faceswap/\"\n",
        "  print(\"The installation is local. Using local directory.\")\n",
        "#set variables end\n",
        " \n",
        "   #fit training args: -nw -nf\n",
        " \n",
        "!python3 '{path}'/faceswap.py train \\\n",
        "  -A '{input_a}' \\\n",
        "  -B '{input_b}' \\\n",
        "  -m '{model_dir}' \\\n",
        "  -t '{trainer_type}' \\\n",
        "  -bs '{batch_num}' \\\n",
        "  -it '{num_iterations}' \\\n",
        "  -s '{save_every}' \\\n",
        "  -ss '{save_model_every}' \\\n",
        "  -tia '{input_a}' \\\n",
        "  -tib '{input_b}' \\\n",
        "  -to '{timelapse_dir}' \\\n",
        "  -w "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "YnHtwcncTuOd"
      },
      "source": [
        "#@title Training with No Warp\n",
        "#@markdown Disabling the warp can let you achieve extra sharpness. Start this training sequence if you think your model has trained enough(*judge by the previews*).\n",
        "#@markdown Your variables are saved from the training session above. If not, start your normal training sequence first, then terminate it.\n",
        "\n",
        "drive_install = True #@param {type:\"boolean\"}\n",
        "if drive_install:\n",
        "  path = \"/content/drive/My Drive/faceswap/\"\n",
        "  print(\"The installation is on Google Drive. Using Drive directory.\")\n",
        "if not drive_install:\n",
        "  path = \"/content/faceswap/\"\n",
        "  print(\"The installation is local. Using local directory.\")\n",
        "\n",
        "!python3 '{path}'/faceswap.py train \\\n",
        "  -A '{input_a}' \\\n",
        "  -B '{input_b}' \\\n",
        "  -m '{model_dir}' \\\n",
        "  -t '{trainer_type}' \\\n",
        "  -bs '{batch_num}' \\\n",
        "  -it '{num_iterations}' \\\n",
        "  -s '{save_every}' \\\n",
        "  -ss '{save_model_every}' \\\n",
        "  -tia '{input_a}' \\\n",
        "  -tib '{input_b}' \\\n",
        "  -to '{timelapse_dir}' \\\n",
        "  -w \\\n",
        "  -nw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORdq3Tx1lgAu",
        "cellView": "form"
      },
      "source": [
        "#@title Convert\n",
        "#@markdown After your model is trained enough, you can convert your final result. Fill the variables and run the code yet another time.\n",
        "\n",
        "#variables set\n",
        "input_video = \"/content/dir/videoA.mp4\" #@param {type:\"string\"}\n",
        "output_dir = \"/content/dir/output\" #@param {type:\"string\"}\n",
        "alingments = \"/content/dir/alignments.fsa\" #@param {type:\"string\"}\n",
        "model_dir = \"/content/dir/your_model\" #@param {type:\"string\"}\n",
        "color_adj = \"\" #@param [\"\", \"avg-color\", \"color-transfer\", \"manual-balance\", \"match-hist\", \"seamless-clone\"]\n",
        "mask = \"\" #@param [\"\", \"components\", \"extended\", \"unet-dfl\", \"vgg-clear\", \"vgg-obstructed\", \"predicted\"]\n",
        "writer = \"ffmpeg\" #@param [\"ffmpeg\", \"gif\", \"opencv\", \"pillow\"]\n",
        "output_scale = 100 #@param {type:\"slider\", min:25, max:400, step:5}\n",
        "drive_install = True #@param {type:\"boolean\"}\n",
        "if drive_install:\n",
        "  path = \"/content/drive/My Drive/faceswap/\"\n",
        "  print(\"The installation is on Google Drive. Using Drive directory.\")\n",
        "if not drive_install:\n",
        "  path = \"/content/faceswap/\"\n",
        "  print(\"The installation is local. Using local directory.\")\n",
        "#@markdown \"Predicted\" mask type is used only if you trained your own mask using \"Learn Mask\". If you're not sure, use \"Components\" or \"Extended\" as they are used by default.\n",
        "#variables end\n",
        "\n",
        "!python3 faceswap\\faceswap.py convert \\\n",
        "-i '{input_video}' \\\n",
        "-o '{output_dir}' \\\n",
        "-al '{alignments}' \\\n",
        "-m '{model_dir}' \\\n",
        "-c '{color_adj}' \\\n",
        "-M '{mask}' \\\n",
        "-w '{writer}' \\\n",
        "-osc '{output_scale}' -l 0.4 -j 0 -L INFO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqe2bTXRKjGC"
      },
      "source": [
        "# Various Tools\n",
        "\n",
        "*Use if necessary.*\n",
        "\n",
        "Faceswap provides a couple of handy tools for you to make your dataset managment easier. Here you can do whatever you want(*limited to the \"Manual\" tool as it requires a desktop environment*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "z8dvMuZtLDxJ"
      },
      "source": [
        "#@title Alignments\n",
        "#@markdown This tool allows you to perform various actions to the alignments file. It is usually stored next to the video source file.\n",
        "#@markdown You would only need \"Remove Faces\" job for most of the time.\n",
        "#@markdown If you wish to use any other tool, proceed to the guides to find out how to use some of them.\n",
        "#set variables\n",
        "job = \"remove-faces\" #@param [\"draw\", \"extract\", \"missing-alignments\", \"missing-frames\", \"multi-faces\", \"no-faces\", \"remove-faces\", \"rename\", \"sort\"]\n",
        "alignments = \"/content/dir/alignments.fsa\" #@param {type:\"string\"}\n",
        "faces = \"/content/dir/A or B\" #@param {type:\"string\"}\n",
        "drive_install = True #@param {type:\"boolean\"}\n",
        "if drive_install:\n",
        "  path = \"/content/drive/My Drive/faceswap/\"\n",
        "  print(\"The installation is on Google Drive. Using Drive directory.\")\n",
        "if not drive_install:\n",
        "  path = \"/content/faceswap/\"\n",
        "  print(\"The installation is local. Using local directory.\")\n",
        "#end variables\n",
        "!python3 faceswap\\tools.py alignments \\\n",
        "-j '{job}' \\\n",
        "-o console \\\n",
        "-a '{alignments}' \\\n",
        "-fc '{faces}' \\\n",
        "-een 1 -sz 512 -L INFO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBRusquuOm7Z",
        "cellView": "form"
      },
      "source": [
        "#@title Effmpeg\n",
        "#@markdown Generate videos for timelapse(*usually this tool is required for that*). **Strongly** recommended to use BRU first to rename each timestamp to start from 000000.jpeg.\n",
        "\n",
        "#@markdown **NOT DONE YET, DO NOT USE**\n",
        "#set variables\n",
        "\n",
        "#end variables\n",
        "!python3 faceswap\\tools.py effmpeg \\\n",
        "-a gen-vid \\\n",
        "-i input \\\n",
        "-o output \\\n",
        "-fps 5 \\\n",
        "-ef .png \\\n",
        "-s 00:00:00 \\\n",
        "-e 00:00:00 \\\n",
        "-d 00:00:00 -sc 1920x1080 -L INFO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Jn7-NAn5cI"
      },
      "source": [
        "# Configuration\n",
        " \n",
        "Here you can customize your configuration for Extraction, Training and Convertion.\n",
        "\n",
        "**NOT WORKING AND YET TO BE IMPLEMENTED**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ugLignRfQjED"
      },
      "source": [
        "#@title Extract\n",
        "#@markdown Tick this only if you installed Faceswap on Drive. It will lead to the Faceswap directory.\n",
        "drive_install = False #@param {type:\"boolean\"}\n",
        "if drive_install:\n",
        "  path = \"/content/drive/My Drive/faceswap/\"\n",
        "  print(\"The installation is on Google Drive. Using Drive directory.\")\n",
        "if not drive_install:\n",
        "  path = \"/content/faceswap/\"\n",
        "  print(\"The installation is local. Using local directory.\")\n",
        "%cd '{path}'config\n",
        "#@markdown Then, proceed to the next code snippet to apply your configuration."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MsBEi-qsXqfe"
      },
      "source": [
        "%%writefile extract.ini\n",
        "#@markdown Global:\n",
        "\n",
        "[global]\n",
        "# OPTIONS THAT APPLY TO ALL EXTRACTION PLUGINS\n",
        "\n",
        "# [Nvidia Only]. Enable the Tensorflow GPU `allow_growth` configuration option. This option prevents\n",
        "# Tensorflow from allocating all of the GPU VRAM at launch but can lead to higher VRAM fragmentation\n",
        "# and slower performance. Should only be enabled if you are having problems running extraction.\n",
        "# \n",
        "# Choose from: True, False\n",
        "# [Default: False] \n",
        "allow_growth = False #@param {type:\"boolean\"}\n",
        "#@markdown FAN Aligner:\n",
        "[align.fan]\n",
        "# FAN ALIGNER OPTIONS.\n",
        "# FAST ON GPU, SLOW ON CPU. BEST ALIGNER.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "#     - AMD users: A batchsize of 8 requires about 4 GB vram.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 12]\n",
        "batch-size = 5 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "#@markdown Cv2-Dnn Detector:\n",
        "[detect.cv2_dnn]\n",
        "# CV2 DNN DETECTOR OPTIONS.\n",
        "# A CPU ONLY EXTRACTOR, IS THE LEAST RELIABLE, BUT USES LEAST RESOURCES AND RUNS FAST ON CPU. USE THIS\n",
        "# IF NOT USING A GPU AND TIME IS IMPORTANT\n",
        "\n",
        "# The confidence level at which the detector has succesfully found a face.\n",
        "# Higher levels will be more discriminating, lower levels will have more false positives.\n",
        "# \n",
        "# Select an integer between 25 and 100\n",
        "# [Default: 50]\n",
        "confidence = 70 #@param {type:\"slider\", min:25, max:100, step:5}\n",
        "#@markdown Mtcnn Detector:\n",
        "[detect.mtcnn]\n",
        "# MTCNN DETECTOR OPTIONS.\n",
        "# FAST ON GPU, SLOW ON CPU. USES FEWER RESOURCES THAN OTHER GPU DETECTORS BUT CAN OFTEN RETURN MORE\n",
        "# FALSE POSITIVES.\n",
        "\n",
        "# The minimum size of a face (in pixels) to be accepted as a positive match.\n",
        "# Lower values use significantly more VRAM and will detect more false positives.\n",
        "# \n",
        "# Select an integer between 20 and 1000\n",
        "# [Default: 20]\n",
        "minsize = 710 #@param {type:\"slider\", min:20, max:1000, step:10}\n",
        "\n",
        "# The scale factor for the image pyramid.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 0.9\n",
        "# [Default: 0.709]\n",
        "scalefactor = 0.406 #@param {type:\"slider\", min:0.1, max:0.9, step:0.001}\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 8]\n",
        "batch-size = 4 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "\n",
        "# First stage threshold for face detection. This stage obtains face candidates.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 0.9\n",
        "# [Default: 0.6]\n",
        "threshold_1 = 0.79 #@param {type:\"slider\", min:0.1, max:0.9, step:0.01}\n",
        "\n",
        "# Second stage threshold for face detection. This stage refines face candidates.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 0.9\n",
        "# [Default: 0.7]\n",
        "threshold_2 = 0.43 #@param {type:\"slider\", min:0.1, max:0.9, step:0.01}\n",
        "\n",
        "# Third stage threshold for face detection. This stage further refines face candidates.\n",
        "# \n",
        "# Select a decimal number between 0.1 and 0.9\n",
        "# [Default: 0.7]\n",
        "threshold_3 = 0.85 #@param {type:\"slider\", min:0.1, max:0.9, step:0.01}\n",
        "#@markdown S3FD Detector:\n",
        "[detect.s3fd]\n",
        "# S3FD DETECTOR OPTIONS.\n",
        "# FAST ON GPU, SLOW ON CPU. CAN DETECT MORE FACES AND FEWER FALSE POSITIVES THAN OTHER GPU DETECTORS,\n",
        "# BUT IS A LOT MORE RESOURCE INTENSIVE.\n",
        "\n",
        "# The confidence level at which the detector has succesfully found a face.\n",
        "# Higher levels will be more discriminating, lower levels will have more false positives.\n",
        "# \n",
        "# Select an integer between 25 and 100\n",
        "# [Default: 70]\n",
        "confidence = 70 #@param {type:\"slider\", min:25, max:100, step:5}\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "#     - AMD users: A batchsize of 8 requires about 2 GB vram.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 4]\n",
        "batch-size = 4 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "#@markdown Unet_DFL Mask:\n",
        "[mask.unet_dfl]\n",
        "# UNET_DFL OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES.\n",
        "# THE MASK MODEL HAS BEEN TRAINED BY COMMUNITY MEMBERS. INSERT MORE COMMENTARY ON TESTING HERE.\n",
        "# PROFILE FACES MAY RESULT IN SUB-PAR PERFORMANCE.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 8]\n",
        "batch-size = 8 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "#@markdown VGG-Clear Mask:\n",
        "[mask.vgg_clear]\n",
        "# VGG_CLEAR OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES CLEAR OF\n",
        "# OBSTRUCTIONS.\n",
        "# PROFILE FACES AND OBSTRUCTIONS MAY RESULT IN SUB-PAR PERFORMANCE.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 6]\n",
        "batch-size = 6 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "#@markdown VGG-Obstructed Mask:\n",
        "[mask.vgg_obstructed]\n",
        "# VGG_OBSTRUCTED OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES.\n",
        "# THE MASK MODEL HAS BEEN SPECIFICALLY TRAINED TO RECOGNIZE SOME FACIAL OBSTRUCTIONS (HANDS AND\n",
        "# EYEGLASSES). PROFILE FACES MAY RESULT IN SUB-PAR PERFORMANCE.\n",
        "\n",
        "# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too\n",
        "# high can harm performance.\n",
        "# \n",
        "#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will\n",
        "# \t\tautomatically be lowered.\n",
        "# \n",
        "# Select an integer between 1 and 64\n",
        "# [Default: 2]\n",
        "batch-size = 2 #@param {type:\"slider\", min:1, max:64, step:1}\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}